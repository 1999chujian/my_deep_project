{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100\n",
      "Number of unique input tokens: 46\n",
      "Number of unique output tokens: 149\n",
      "Max sequence length for inputs: 9\n",
      "Max sequence length for outputs: 11\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 100  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 512  # Latent dimensionality of the encoding space.\n",
    "num_samples = 100  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'cmn.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, '!': 2, '。': 3, '一': 4, '上': 5, '下': 6, '不': 7, '世': 8, '个': 9, '为': 10, '么': 11, '乾': 12, '了': 13, '事': 14, '人': 15, '什': 16, '他': 17, '付': 18, '们': 19, '会': 20, '住': 21, '你': 22, '來': 23, '信': 24, '們': 25, '儿': 26, '入': 27, '公': 28, '关': 29, '再': 30, '冷': 31, '出': 32, '别': 33, '到': 34, '前': 35, '力': 36, '加': 37, '动': 38, '努': 39, '去': 40, '友': 41, '可': 42, '吃': 43, '同': 44, '后': 45, '吧': 46, '听': 47, '吻': 48, '呆': 49, '告': 50, '和': 51, '善': 52, '嗨': 53, '嘴': 54, '坚': 55, '失': 56, '她': 57, '好': 58, '姆': 59, '始': 60, '它': 61, '完': 62, '定': 63, '就': 64, '帮': 65, '干': 66, '平': 67, '开': 68, '弃': 69, '当': 70, '往': 71, '很': 72, '得': 73, '心': 74, '忘': 75, '忙': 76, '快': 77, '意': 78, '我': 79, '找': 80, '把': 81, '抓': 82, '抱': 83, '拿': 84, '持': 85, '放': 86, '是': 87, '来': 88, '杯': 89, '欢': 90, '气': 91, '汤': 92, '沒': 93, '没': 94, '泳': 95, '洗': 96, '清': 97, '游': 98, '滾': 99, '点': 100, '玩': 101, '生': 102, '用': 103, '留': 104, '病': 105, '的': 106, '相': 107, '着': 108, '知': 109, '确': 110, '立': 111, '等': 112, '管': 113, '系': 114, '美': 115, '老': 116, '联': 117, '能': 118, '见': 119, '让': 120, '试': 121, '谁': 122, '赢': 123, '走': 124, '起': 125, '趕': 126, '趴': 127, '跑': 128, '跳': 129, '辞': 130, '迎': 131, '进': 132, '迷': 133, '退': 134, '道': 135, '醒': 136, '錢': 137, '铐': 138, '閉': 139, '開': 140, '门': 141, '问': 142, '随': 143, '静': 144, '飽': 145, '！': 146, '，': 147, '？': 148}\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "print(target_token_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 46)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 149)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 512), (None, 1144832     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1355776     input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 149)    76437       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,577,045\n",
      "Trainable params: 2,577,045\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 2s 22ms/step - loss: 2.3114 - acc: 0.0030 - val_loss: 2.4286 - val_acc: 0.1182\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 2.2745 - acc: 0.1222 - val_loss: 2.3151 - val_acc: 0.0909\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 2.1566 - acc: 0.0909 - val_loss: 1.9057 - val_acc: 0.0909\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.6985 - acc: 0.0909 - val_loss: 2.2805 - val_acc: 0.0727\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.9672 - acc: 0.0636 - val_loss: 1.8786 - val_acc: 0.1364\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.6882 - acc: 0.1576 - val_loss: 1.9074 - val_acc: 0.1000\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.7073 - acc: 0.1152 - val_loss: 1.9543 - val_acc: 0.1000\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.6807 - acc: 0.1121 - val_loss: 2.0533 - val_acc: 0.1000\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.7120 - acc: 0.1111 - val_loss: 2.0305 - val_acc: 0.1000\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.6798 - acc: 0.1152 - val_loss: 1.9564 - val_acc: 0.1000\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.6232 - acc: 0.1152 - val_loss: 1.9138 - val_acc: 0.1000\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.5943 - acc: 0.1172 - val_loss: 1.8959 - val_acc: 0.1091\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.5680 - acc: 0.1172 - val_loss: 1.8897 - val_acc: 0.1091\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.5193 - acc: 0.1172 - val_loss: 1.9123 - val_acc: 0.1273\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.4707 - acc: 0.1273 - val_loss: 1.9745 - val_acc: 0.1364\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.4544 - acc: 0.1545 - val_loss: 2.0261 - val_acc: 0.1364\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.4416 - acc: 0.1586 - val_loss: 2.0284 - val_acc: 0.1455\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.4041 - acc: 0.1545 - val_loss: 1.9990 - val_acc: 0.1364\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.3764 - acc: 0.1485 - val_loss: 1.9841 - val_acc: 0.1364\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.3638 - acc: 0.1434 - val_loss: 2.0497 - val_acc: 0.1364\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.3310 - acc: 0.1596 - val_loss: 2.1259 - val_acc: 0.1273\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.3077 - acc: 0.1606 - val_loss: 2.1819 - val_acc: 0.1273\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.2628 - acc: 0.1636 - val_loss: 2.2328 - val_acc: 0.1273\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.2511 - acc: 0.1717 - val_loss: 2.2838 - val_acc: 0.1273\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.2071 - acc: 0.1687 - val_loss: 2.2877 - val_acc: 0.1182\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1878 - acc: 0.1737 - val_loss: 2.2719 - val_acc: 0.1000\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1534 - acc: 0.1737 - val_loss: 2.3091 - val_acc: 0.1182\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 1.1259 - acc: 0.1747 - val_loss: 2.4100 - val_acc: 0.1182\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1008 - acc: 0.1707 - val_loss: 2.4307 - val_acc: 0.1000\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0672 - acc: 0.1818 - val_loss: 2.3699 - val_acc: 0.1000\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0500 - acc: 0.1960 - val_loss: 2.4457 - val_acc: 0.0909\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0173 - acc: 0.1899 - val_loss: 2.5086 - val_acc: 0.0909\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.9749 - acc: 0.1929 - val_loss: 2.5340 - val_acc: 0.0909\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.9562 - acc: 0.2091 - val_loss: 2.6634 - val_acc: 0.0818\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.9082 - acc: 0.2081 - val_loss: 2.8113 - val_acc: 0.1000\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.8736 - acc: 0.2141 - val_loss: 2.7944 - val_acc: 0.0909\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.8399 - acc: 0.2394 - val_loss: 2.7220 - val_acc: 0.1000\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.8027 - acc: 0.2535 - val_loss: 2.8459 - val_acc: 0.0909\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.7565 - acc: 0.2556 - val_loss: 2.9368 - val_acc: 0.1000\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.7291 - acc: 0.2626 - val_loss: 2.9455 - val_acc: 0.1091\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.7107 - acc: 0.2545 - val_loss: 2.8150 - val_acc: 0.1364\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.7154 - acc: 0.2768 - val_loss: 3.1246 - val_acc: 0.1000\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.8402 - acc: 0.2121 - val_loss: 3.0491 - val_acc: 0.1182\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.7178 - acc: 0.2485 - val_loss: 3.1423 - val_acc: 0.1182\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.6735 - acc: 0.2737 - val_loss: 3.1346 - val_acc: 0.1364\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.6421 - acc: 0.2970 - val_loss: 3.1010 - val_acc: 0.1364\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5990 - acc: 0.2949 - val_loss: 3.1483 - val_acc: 0.1091\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5688 - acc: 0.2889 - val_loss: 3.2790 - val_acc: 0.1000\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5343 - acc: 0.3081 - val_loss: 3.3944 - val_acc: 0.1000\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4953 - acc: 0.3283 - val_loss: 3.4256 - val_acc: 0.1182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4740 - acc: 0.3303 - val_loss: 3.3632 - val_acc: 0.1091\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4273 - acc: 0.3606 - val_loss: 3.2782 - val_acc: 0.1273\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4017 - acc: 0.3687 - val_loss: 3.2727 - val_acc: 0.1273\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3721 - acc: 0.3788 - val_loss: 3.3487 - val_acc: 0.1182\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3441 - acc: 0.3828 - val_loss: 3.4222 - val_acc: 0.1091\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.3135 - acc: 0.3939 - val_loss: 3.4208 - val_acc: 0.1182\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2864 - acc: 0.4000 - val_loss: 3.4395 - val_acc: 0.1273\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2659 - acc: 0.4040 - val_loss: 3.5305 - val_acc: 0.1273\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.2344 - acc: 0.4182 - val_loss: 3.6025 - val_acc: 0.1182\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2175 - acc: 0.4242 - val_loss: 3.6184 - val_acc: 0.1182\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1982 - acc: 0.4303 - val_loss: 3.6769 - val_acc: 0.1182\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1771 - acc: 0.4333 - val_loss: 3.7692 - val_acc: 0.1182\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1617 - acc: 0.4333 - val_loss: 3.8308 - val_acc: 0.1182\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1470 - acc: 0.4374 - val_loss: 3.8648 - val_acc: 0.1182\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1338 - acc: 0.4404 - val_loss: 3.9174 - val_acc: 0.1182\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1205 - acc: 0.4455 - val_loss: 3.9442 - val_acc: 0.1182\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1103 - acc: 0.4434 - val_loss: 3.9463 - val_acc: 0.1182\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0993 - acc: 0.4424 - val_loss: 3.9736 - val_acc: 0.1182\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0901 - acc: 0.4485 - val_loss: 4.0378 - val_acc: 0.1273\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0841 - acc: 0.4455 - val_loss: 4.0981 - val_acc: 0.1182\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0767 - acc: 0.4485 - val_loss: 4.1316 - val_acc: 0.1091\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0712 - acc: 0.4485 - val_loss: 4.1513 - val_acc: 0.1000\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0655 - acc: 0.4485 - val_loss: 4.1660 - val_acc: 0.1000\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0606 - acc: 0.4495 - val_loss: 4.1898 - val_acc: 0.1000\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0558 - acc: 0.4505 - val_loss: 4.2209 - val_acc: 0.1000\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0522 - acc: 0.4505 - val_loss: 4.2494 - val_acc: 0.1000\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0475 - acc: 0.4515 - val_loss: 4.2713 - val_acc: 0.0909\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0444 - acc: 0.4515 - val_loss: 4.2848 - val_acc: 0.0909\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0412 - acc: 0.4505 - val_loss: 4.2953 - val_acc: 0.0909\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0386 - acc: 0.4525 - val_loss: 4.3107 - val_acc: 0.0818\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0375 - acc: 0.4515 - val_loss: 4.3366 - val_acc: 0.0818\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0381 - acc: 0.4495 - val_loss: 4.3817 - val_acc: 0.0909\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0459 - acc: 0.4475 - val_loss: 4.3999 - val_acc: 0.0909\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0332 - acc: 0.4515 - val_loss: 4.3949 - val_acc: 0.0909\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0377 - acc: 0.4505 - val_loss: 4.4307 - val_acc: 0.1000\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0335 - acc: 0.4505 - val_loss: 4.4290 - val_acc: 0.1000\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0306 - acc: 0.4505 - val_loss: 4.4273 - val_acc: 0.0818\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0305 - acc: 0.4525 - val_loss: 4.4128 - val_acc: 0.0909\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0273 - acc: 0.4505 - val_loss: 4.4009 - val_acc: 0.0909\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0290 - acc: 0.4515 - val_loss: 4.4352 - val_acc: 0.1000\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0244 - acc: 0.4515 - val_loss: 4.4386 - val_acc: 0.1000\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0229 - acc: 0.4525 - val_loss: 4.4433 - val_acc: 0.1000\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0252 - acc: 0.4505 - val_loss: 4.4660 - val_acc: 0.0909\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0216 - acc: 0.4535 - val_loss: 4.4457 - val_acc: 0.0909\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0219 - acc: 0.4525 - val_loss: 4.4013 - val_acc: 0.0909\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0200 - acc: 0.4535 - val_loss: 4.3783 - val_acc: 0.1000\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0200 - acc: 0.4535 - val_loss: 4.3916 - val_acc: 0.1091\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0193 - acc: 0.4535 - val_loss: 4.4015 - val_acc: 0.1000\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0194 - acc: 0.4535 - val_loss: 4.3968 - val_acc: 0.1000\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0182 - acc: 0.4535 - val_loss: 4.3789 - val_acc: 0.1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "opt = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [ state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.      \n",
      "\t你好。\n",
      "\t\t\t\t\t\t\n",
      "你好。\n",
      "\t\t\t\t\t\t\t\n",
      "Hi.\n",
      "嗨。\n",
      "\n",
      "Hi.\n",
      "嗨。\n",
      "\n",
      "Run.\n",
      "你用跑的。\n",
      "\n",
      "Wait!\n",
      "等等！\n",
      "\n",
      "Hello!\n",
      "你好。\n",
      "\n",
      "I try.\n",
      "让我来。\n",
      "\n",
      "I won!\n",
      "我赢了。\n",
      "\n",
      "Oh no!\n",
      "不会吧。\n",
      "\n",
      "Cheers!\n",
      "乾杯!\n",
      "\n",
      "He ran.\n",
      "他跑了。\n",
      "\n",
      "Hop in.\n",
      "跳进来。\n",
      "\n",
      "I lost.\n",
      "我迷失了。\n",
      "\n",
      "I quit.\n",
      "我退出。\n",
      "\n",
      "I'm OK.\n",
      "我沒事。\n",
      "\n",
      "Listen.\n",
      "听着。\n",
      "\n",
      "No way!\n",
      "不可能！\n",
      "\n",
      "No way!\n",
      "不可能！\n",
      "\n",
      "Really?\n",
      "你确定？\n",
      "\n",
      "Try it.\n",
      "试试吧。\n",
      "\n",
      "We try.\n",
      "我们来试试。\n",
      "\n",
      "Why me?\n",
      "为什么是我？\n",
      "\n",
      "Ask Tom.\n",
      "去问汤姆。\n",
      "\n",
      "Be calm.\n",
      "冷静点。\n",
      "\n",
      "Be fair.\n",
      "公平点。\n",
      "\n",
      "Be kind.\n",
      "友善点。\n",
      "\n",
      "Be nice.\n",
      "和气点。\n",
      "\n",
      "Call me.\n",
      "联系我。\n",
      "\n",
      "Call us.\n",
      "联系我们。\n",
      "\n",
      "Come in.\n",
      "进来。\n",
      "\n",
      "Get Tom.\n",
      "找到汤姆。\n",
      "\n",
      "Get out!\n",
      "滾出去！\n",
      "\n",
      "Go away!\n",
      "走開！\n",
      "\n",
      "Go away!\n",
      "走開！\n",
      "\n",
      "Go away.\n",
      "走開！\n",
      "\n",
      "Goodbye!\n",
      "再见！\n",
      "\n",
      "Goodbye!\n",
      "再见！\n",
      "\n",
      "Hang on!\n",
      "等一下！\n",
      "\n",
      "He came.\n",
      "他来了。\n",
      "\n",
      "He runs.\n",
      "他跑。\n",
      "\n",
      "Help me.\n",
      "帮我一下。\n",
      "\n",
      "Hold on.\n",
      "坚持。\n",
      "\n",
      "Hug Tom.\n",
      "抱抱汤姆！\n",
      "\n",
      "I agree.\n",
      "我同意。\n",
      "\n",
      "I'm ill.\n",
      "我生病了。\n",
      "\n",
      "I'm old.\n",
      "我老了。\n",
      "\n",
      "It's OK.\n",
      "没关系。\n",
      "\n",
      "It's me.\n",
      "是我。\n",
      "\n",
      "Join us.\n",
      "来加入我们吧。\n",
      "\n",
      "Keep it.\n",
      "留着吧。\n",
      "\n",
      "Kiss me.\n",
      "吻我。\n",
      "\n",
      "Perfect!\n",
      "完美！\n",
      "\n",
      "See you.\n",
      "再见！\n",
      "\n",
      "Shut up!\n",
      "閉嘴！\n",
      "\n",
      "Skip it.\n",
      "不管它。\n",
      "\n",
      "Take it.\n",
      "拿走吧。\n",
      "\n",
      "Wake up!\n",
      "醒醒！\n",
      "\n",
      "Wash up.\n",
      "去清洗一下。\n",
      "\n",
      "We know.\n",
      "我们知道。\n",
      "\n",
      "Welcome.\n",
      "欢迎。\n",
      "\n",
      "Who won?\n",
      "谁赢了？\n",
      "\n",
      "Why not?\n",
      "为什么不？\n",
      "\n",
      "You run.\n",
      "你跑。\n",
      "\n",
      "Back off.\n",
      "往后退点。\n",
      "\n",
      "Be still.\n",
      "静静的，别动。\n",
      "\n",
      "Cuff him.\n",
      "把他铐上。\n",
      "\n",
      "Drive on.\n",
      "往前开。\n",
      "\n",
      "Get away!\n",
      "走開！\n",
      "\n",
      "Get away!\n",
      "走開！\n",
      "\n",
      "Get down!\n",
      "趴下！\n",
      "\n",
      "Get lost!\n",
      "滾！\n",
      "\n",
      "Get real.\n",
      "醒醒吧。\n",
      "\n",
      "Grab Tom.\n",
      "抓住汤姆。\n",
      "\n",
      "Grab him.\n",
      "抓住他。\n",
      "\n",
      "Have fun.\n",
      "玩得開心。\n",
      "\n",
      "He tries.\n",
      "他来试试。\n",
      "\n",
      "Humor me.\n",
      "你就随了我的意吧。\n",
      "\n",
      "Hurry up.\n",
      "趕快!\n",
      "\n",
      "Hurry up.\n",
      "趕快!\n",
      "\n",
      "I forgot.\n",
      "我忘了。\n",
      "\n",
      "I resign.\n",
      "我放弃。\n",
      "\n",
      "I'll pay.\n",
      "我來付錢。\n",
      "\n",
      "I'm busy.\n",
      "我很忙。\n",
      "\n",
      "I'm cold.\n",
      "我冷。\n",
      "\n",
      "I'm fine.\n",
      "我很好。\n",
      "\n",
      "I'm full.\n",
      "我吃飽了。\n",
      "\n",
      "I'm sick.\n",
      "我病了。\n",
      "\n",
      "I'm sick.\n",
      "我病了。\n",
      "\n",
      "Leave me.\n",
      "让我一个人呆会儿。\n",
      "\n",
      "Let's go!\n",
      "走吧。\n",
      "\n",
      "Let's go!\n",
      "走吧。\n",
      "\n",
      "Let's go!\n",
      "走吧。\n",
      "\n",
      "Look out!\n",
      "不可能！\n",
      "\n",
      "She runs.\n",
      "去清洗！\n",
      "\n",
      "Stand up.\n",
      "等等！\n",
      "\n",
      "They won.\n",
      "去清洗下。\n",
      "\n",
      "Tom died.\n",
      "进进来。\n",
      "\n",
      "Tom quit.\n",
      "进进来。\n",
      "\n",
      "Tom swam.\n",
      "进进来。\n",
      "\n",
      "Trust me.\n",
      "试试吧。\n",
      "\n",
      "Try hard.\n",
      "试试吧。\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-caf3ad4da9c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mh1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtarget_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_token_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "print(''.join([input_characters[np.argmax(i)] for i in encoder_input_data[1]]))\n",
    "print(''.join([target_characters[np.argmax(i)] for i in decoder_input_data[1]]))\n",
    "print(''.join([target_characters[np.argmax(i)] for i in decoder_target_data[1]]))\n",
    "\n",
    "for k in range(100):\n",
    "    test_data = encoder_input_data[k:k+1]\n",
    "    h1, c1 = encoder_model.predict(test_data)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1\n",
    "    outputs = []\n",
    "    while True:\n",
    "        output_tokens, h1, c1 = decoder_model.predict([target_seq, h1, c1])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        outputs.append(sampled_token_index)\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        if sampled_token_index == target_token_index['\\n'] or len(outputs) > 20: break\n",
    "    \n",
    "    print(input_texts[k])\n",
    "    print(''.join([target_characters[i] for i in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
