{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于seq2seq的中英文翻译系统\n",
    "## 1. 项目背景\n",
    "之前我们利用lstm进行建模，设计了一个自动生成莫言小说的模型，这次想要利用rnn的特点搭建一个中英文的翻译系统。传统的RNN输入和输出长度要一致，而seq2seq在RNN的基础上进行改进，实现了变长序列的输入和输出，广泛的应用在了机器翻译、对话系统、文本摘要等领域。 \n",
    "- 代码参考：https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "## 2. 项目数据\n",
    "项目数据使用中英文翻译数据集，来实现字符级的seq2seq模型的训练。 \n",
    "该文件来自于:http://www.manythings.org/anki/\n",
    "\n",
    "内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Try hard.\\t努力。']\n"
     ]
    }
   ],
   "source": [
    "# ========读取原始数据========\n",
    "with open('cmn.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "data = data[:100]\n",
    "print(data[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据处理\n",
    "### 3.1 生成字典\n",
    "我们需要将汉字和英文映射为能够输入到模型中的数字信息，就需要建立一个映射关系，需要生成汉字和数字互相映射的字典。\n",
    "- 我们将英文按照每个字母对应一个id\n",
    "- 我们将中文按照每一个汉字对应一个id\n",
    "- **注意增加：**\n",
    "    1. 未知符号：UNK\n",
    "    2. 补齐符号：PAD\n",
    "    3. 开始符号：GO\n",
    "    4. 结束符号：EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文数据: ['Hi.', 'Hi.', 'Run.', 'Wait!', 'Hello!', 'I try.', 'I won!', 'Oh no!', 'Cheers!', 'He ran.']\n",
      "中文数据: ['\\t嗨。\\n', '\\t你好。\\n', '\\t你用跑的。\\n', '\\t等等！\\n', '\\t你好。\\n', '\\t让我来。\\n', '\\t我赢了。\\n', '\\t不会吧。\\n', '\\t乾杯!\\n', '\\t他跑了。\\n']\n",
      "\t\n",
      "嗨\n",
      "。\n",
      "\n",
      "\n",
      "英文字典:\n",
      " {'L': 0, 'N': 1, 'w': 2, 'I': 3, 'T': 4, 'd': 5, 't': 6, 'B': 7, 'c': 8, 'm': 9, 'A': 10, '.': 11, 'l': 12, 'P': 13, 'S': 14, \"'\": 15, 'D': 16, 'G': 17, 'H': 18, 'f': 19, 's': 20, 'a': 21, 'i': 22, 'Y': 23, 'n': 24, '!': 25, 'b': 26, 'O': 27, 'g': 28, 'h': 29, 'k': 30, 'v': 31, 'r': 32, 'K': 33, 'R': 34, 'C': 35, 'W': 36, 'u': 37, 'p': 38, 'y': 39, 'J': 40, 'o': 41, 'q': 42, 'e': 43, '?': 44, ' ': 45}\n",
      "中文字典共计:\n",
      " {'清': 0, '往': 1, '帮': 2, '玩': 3, '出': 4, '什': 5, '前': 6, '入': 7, '别': 8, '没': 9, '公': 10, '告': 11, '滾': 12, '欢': 13, '坚': 14, '退': 15, '醒': 16, '个': 17, '跳': 18, '老': 19, '关': 20, '干': 21, '随': 22, '放': 23, '病': 24, '再': 25, '下': 26, '是': 27, '了': 28, '系': 29, '善': 30, '它': 31, '杯': 32, '世': 33, '？': 34, '开': 35, '赢': 36, '吃': 37, '呆': 38, '忙': 39, '他': 40, '汤': 41, '來': 42, '\\n': 43, '\\t': 44, '谁': 45, '好': 46, '！': 47, '我': 48, '相': 49, '游': 50, '嗨': 51, '等': 52, '定': 53, '们': 54, '友': 55, '忘': 56, '開': 57, '迷': 58, '信': 59, '到': 60, '不': 61, '。': 62, '吧': 63, '和': 64, '辞': 65, '儿': 66, '会': 67, '始': 68, '她': 69, '问': 70, '姆': 71, '抓': 72, '力': 73, '着': 74, '拿': 75, '当': 76, '趴': 77, '进': 78, '能': 79, '人': 80, '后': 81, '铐': 82, '心': 83, '点': 84, '联': 85, '很': 86, '趕': 87, '道': 88, '住': 89, '动': 90, '飽': 91, '閉': 92, '门': 93, '完': 94, '让': 95, '立': 96, '!': 97, '气': 98, '洗': 99, '的': 100, '乾': 101, '付': 102, '听': 103, '用': 104, '努': 105, '迎': 106, '去': 107, '美': 108, '錢': 109, '走': 110, '們': 111, '把': 112, '确': 113, '吻': 114, '事': 115, '找': 116, '失': 117, '，': 118, '么': 119, '得': 120, '同': 121, '知': 122, '静': 123, '一': 124, '快': 125, '嘴': 126, '你': 127, '泳': 128, '见': 129, '管': 130, '就': 131, '沒': 132, '跑': 133, '加': 134, '留': 135, '起': 136, '为': 137, '试': 138, '来': 139, '平': 140, '上': 141, '持': 142, '弃': 143, '意': 144, '生': 145, '抱': 146, '可': 147, '冷': 148}\n"
     ]
    }
   ],
   "source": [
    "# 分割英文数据和中文数据\n",
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "ch_data = ['\\t' + line.split('\\t')[1] + '\\n' for line in data]\n",
    "print('英文数据:', en_data[:10])\n",
    "print('中文数据:', ch_data[:10])\n",
    "for char in ch_data[0]:\n",
    "    print(char)\n",
    "\n",
    "# 分别生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "\n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "\n",
    "print('英文字典:\\n', en2id)\n",
    "print('中文字典共计:\\n', (ch2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 转换输入数据格式\n",
    "建立字典后，将文本数据映射为数字数据形式，并整理为矩阵格式。在生成之前需要考虑训练该模型所需的数据格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 22, 11], [18, 22, 11], [34, 37, 24, 11], [36, 21, 22, 6, 25], [18, 43, 12, 12, 41, 25]]\n",
      "[[44, 51, 62, 43], [44, 127, 46, 62, 43], [44, 127, 104, 133, 100, 62, 43], [44, 52, 52, 47, 43], [44, 127, 46, 62, 43]]\n",
      "[[51, 62, 43], [127, 46, 62, 43], [127, 104, 133, 100, 62, 43], [52, 52, 47, 43], [127, 46, 62, 43]]\n"
     ]
    }
   ],
   "source": [
    "# number data\n",
    "\n",
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] for line in ch_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in ch_data]\n",
    "\n",
    "print(en_num_data[:5])\n",
    "print(ch_num_data[:5])\n",
    "print(de_num_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 整理训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "11\n",
      "(100, 11, 149)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# max length\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)\n",
    "\n",
    "\n",
    "encoder_input_data = [line + [0] * (max_encoder_seq_length-len(line)) for line in en_num_data]\n",
    "\n",
    "# no padding, onehot\n",
    "encoder_input_onehot = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
    "decoder_input_onehot = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_onehot[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_onehot[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======预定义模型参数========\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择与建模\n",
    "### 4.1 encoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ======================================keras model==================================\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Embedding, Masking\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE, embeddings_initializer='uniform', mask_zero=True)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, return_state=True)(emb_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 decoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============decoder=============\n",
    "decoder_inputs = Input(shape=(None, CH_VOCAB_SIZE))\n",
    "\n",
    "#emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=CH_VOCAB_SIZE, mask_zero=True)(decoder_inputs)\n",
    "lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(CH_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "decoder_h1, _, _ = lstm1(decoder_inputs, initial_state=[encoder_state_h1, encoder_state_c1])\n",
    "decoder_outputs = decoder_dense(decoder_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    11776       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 149)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  415744      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 149)    38293       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 991,125\n",
      "Trainable params: 991,125\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 2.3126 - acc: 0.0020 - val_loss: 2.4426 - val_acc: 0.0818\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 2.2898 - acc: 0.1131 - val_loss: 2.4085 - val_acc: 0.0909\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 2.2513 - acc: 0.1081 - val_loss: 2.2953 - val_acc: 0.0909\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 2.1427 - acc: 0.0960 - val_loss: 1.9531 - val_acc: 0.0909\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.8331 - acc: 0.0909 - val_loss: 2.1381 - val_acc: 0.0909\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.8129 - acc: 0.0909 - val_loss: 2.0427 - val_acc: 0.1273\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.7283 - acc: 0.1232 - val_loss: 1.9625 - val_acc: 0.0818\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6709 - acc: 0.0828 - val_loss: 1.9146 - val_acc: 0.0818\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6412 - acc: 0.0859 - val_loss: 1.9081 - val_acc: 0.1091\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6359 - acc: 0.1525 - val_loss: 1.9085 - val_acc: 0.1182\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6225 - acc: 0.1293 - val_loss: 1.9100 - val_acc: 0.1000\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5966 - acc: 0.1182 - val_loss: 1.9229 - val_acc: 0.1000\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5701 - acc: 0.1162 - val_loss: 1.9489 - val_acc: 0.1000\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5511 - acc: 0.1131 - val_loss: 1.9722 - val_acc: 0.1000\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5327 - acc: 0.1131 - val_loss: 1.9779 - val_acc: 0.1000\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5079 - acc: 0.1222 - val_loss: 1.9758 - val_acc: 0.1364\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4917 - acc: 0.1374 - val_loss: 1.9913 - val_acc: 0.1091\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4904 - acc: 0.1354 - val_loss: 2.0112 - val_acc: 0.1091\n",
      "Epoch 19/200\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_onehot], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 搭建预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1])\n",
    "\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "decoder_h1, state_h1, state_c1 = lstm1(decoder_inputs, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "decoder_outputs = decoder_dense(decoder_h1)\n",
    "\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1], \n",
    "                      [decoder_outputs, state_h1, state_c1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 利用预测模型进行翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(100):\n",
    "    test_data = encoder_input_data[k:k+1]\n",
    "    h1, c1 = encoder_model.predict(test_data)\n",
    "    target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "    target_seq[0, 0, ch2id['\\t']] = 1\n",
    "    outputs = []\n",
    "    while True:\n",
    "        output_tokens, h1, c1 = decoder_model.predict([target_seq, h1, c1])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        outputs.append(sampled_token_index)\n",
    "        target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        if sampled_token_index == ch2id['\\n'] or len(outputs) > 20: break\n",
    "    \n",
    "    print(en_data[k])\n",
    "    print(''.join([id2ch[i] for i in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
