{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于seq2seq的中英文翻译系统\n",
    "## 1. 项目背景\n",
    "之前我们利用lstm进行建模，设计了一个自动生成莫言小说的模型，这次想要利用rnn的特点搭建一个中英文的翻译系统。传统的RNN输入和输出长度要一致，而seq2seq在RNN的基础上进行改进，实现了变长序列的输入和输出，广泛的应用在了机器翻译、对话系统、文本摘要等领域。 \n",
    "- 代码参考：https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "## 2. 项目数据\n",
    "项目数据使用英法文翻译数据集，来实现字符级的seq2seq模型的训练。 \n",
    "该文件来自于:http://www.manythings.org/anki/\n",
    "\n",
    "内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Try this.\\tEssayez ceci !']\n"
     ]
    }
   ],
   "source": [
    "# ========读取原始数据========\n",
    "with open('fra.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "data = data[:400]\n",
    "print(data[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据处理\n",
    "### 3.1 生成字典\n",
    "我们需要将汉字和英文映射为能够输入到模型中的数字信息，就需要建立一个映射关系，需要生成汉字和数字互相映射的字典。\n",
    "- 我们将英文按照每个字母对应一个id\n",
    "- 我们将中文按照每一个汉字对应一个id\n",
    "- **注意增加：**\n",
    "    1. 未知符号：UNK\n",
    "    2. 补齐符号：PAD\n",
    "    3. 开始符号：GO\n",
    "    4. 结束符号：EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文数据: ['Go.', 'Run!', 'Run!', 'Fire!', 'Help!', 'Jump.', 'Stop!', 'Stop!', 'Stop!', 'Wait!']\n",
      "中文数据: ['Va !', 'Cours\\u202f!', 'Courez\\u202f!', 'Au feu !', \"À l'aide\\u202f!\", 'Saute.', 'Ça suffit\\u202f!', 'Stop\\u202f!', 'Arrête-toi !', 'Attends !']\n",
      "英文字典:\n",
      " {'__PAD__': 0, '__UNK__': 1, ' ': 2, 'k': 3, 'M': 4, 'H': 5, 's': 6, 'w': 7, 'p': 8, 'c': 9, ',': 10, 'Y': 11, 't': 12, 'h': 13, 'v': 14, 'D': 15, 'G': 16, 'j': 17, 'l': 18, '.': 19, 'R': 20, \"'\": 21, 'o': 22, 'J': 23, 'P': 24, 'b': 25, 'T': 26, 'y': 27, 'K': 28, 'O': 29, 'W': 30, 'N': 31, 'L': 32, 'u': 33, 'e': 34, 'S': 35, 'g': 36, '?': 37, 'f': 38, 'a': 39, 'B': 40, 'C': 41, 'm': 42, 'A': 43, '9': 44, 'z': 45, 'q': 46, 'i': 47, 'n': 48, 'r': 49, '1': 50, 'd': 51, '!': 52, 'F': 53, 'I': 54}\n",
      "法文字典\n",
      ": {'__PAD__': 0, '__UNK__': 1, '__GO__': 2, '__EOS__': 3, ' ': 4, 'V': 5, 'à': 6, 'x': 7, 'E': 8, 'M': 9, 'H': 10, 's': 11, 'p': 12, 'c': 13, ',': 14, 't': 15, 'h': 16, 'Q': 17, 'v': 18, 'D': 19, 'G': 20, 'j': 21, 'î': 22, '\\u2009': 23, 'ê': 24, 'l': 25, '.': 26, 'é': 27, \"'\": 28, 'R': 29, 'o': 30, 'J': 31, 'P': 32, 'b': 33, 'T': 34, 'y': 35, 'O': 36, 'N': 37, 'À': 38, 'L': 39, '-': 40, 'Ç': 41, '’': 42, 'ç': 43, 'u': 44, 'e': 45, 'S': 46, 'g': 47, 'f': 48, '?': 49, 'è': 50, 'û': 51, 'a': 52, 'B': 53, 'C': 54, 'm': 55, 'A': 56, 'z': 57, '9': 58, 'q': 59, 'i': 60, 'â': 61, 'r': 62, 'n': 63, '\\u202f': 64, '1': 65, 'É': 66, 'd': 67, '!': 68, 'F': 69, 'I': 70}\n"
     ]
    }
   ],
   "source": [
    "# 分割英文数据和中文数据\n",
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "fra_data = [line.split('\\t')[1] for line in data]\n",
    "print('英文数据:', en_data[:10])\n",
    "print('中文数据:', fra_data[:10])\n",
    "\n",
    "# 分别生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = ['__PAD__', '__UNK__'] + list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "fra_vocab = set(''.join(fra_data))\n",
    "id2fra = ['__PAD__', '__UNK__', '__GO__', '__EOS__'] + list(fra_vocab)\n",
    "fra2id = {c:i for i,c in enumerate(id2fra)}\n",
    "\n",
    "print('英文字典:\\n', en2id)\n",
    "print('法文字典\\n:', fra2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 转换输入数据格式\n",
    "建立字典后，将文本数据映射为数字数据形式，并整理为矩阵格式。在生成之前需要考虑训练该模型所需的数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 22, 19], [20, 33, 48, 52], [20, 33, 48, 52], [53, 47, 49, 34, 52], [5, 34, 18, 8, 52]]\n",
      "[[2, 5, 52, 4, 68], [2, 54, 30, 44, 62, 11, 64, 68], [2, 54, 30, 44, 62, 45, 57, 64, 68], [2, 56, 44, 4, 48, 45, 44, 4, 68], [2, 38, 4, 25, 28, 52, 60, 67, 45, 64, 68]]\n",
      "[[5, 52, 4, 68, 3], [54, 30, 44, 62, 11, 64, 68, 3], [54, 30, 44, 62, 45, 57, 64, 68, 3], [56, 44, 4, 48, 45, 44, 4, 68, 3], [38, 4, 25, 28, 52, 60, 67, 45, 64, 68, 3]]\n"
     ]
    }
   ],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "fra_num_data = [[fra2id['__GO__']] + [fra2id[ch] for ch in line] for line in fra_data]\n",
    "de_num_data = [[fra2id[fra] for fra in line] + [fra2id['__EOS__']] for line in fra_data]\n",
    "print(en_num_data[:5])\n",
    "print(fra_num_data[:5])\n",
    "print(de_num_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 整理训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "31\n",
      "(400, 31, 71)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in fra_num_data])\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)\n",
    "\n",
    "\n",
    "encoder_input_data = [line + [0] * (max_encoder_seq_length-len(line)) for line in en_num_data]\n",
    "decoder_input_data = [line + [0] * (max_decoder_seq_length-len(line)) for line in fra_num_data]\n",
    "decoder_output_data = [line + [0] * (max_decoder_seq_length-len(line)) for line in de_num_data]\n",
    "decoder_target_data = np.zeros((len(fra_num_data), max_decoder_seq_length, len(fra2id)), dtype='float32')\n",
    "for i in range(len(fra_num_data)):\n",
    "    for j in range(max_decoder_seq_length):\n",
    "        decoder_target_data[i,j,decoder_output_data[i][j]] = 1\n",
    "\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======预定义模型参数========\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "FRA_VOCAB_SIZE = len(fra2id)\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择与建模\n",
    "### 4.1 encoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ======================================keras model==================================\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Embedding\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE, input_length=None, mask_zero=True)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)(emb_inp)\n",
    "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, activation='relu', return_state=True, dropout=0.2)(encoder_h1)\n",
    "encoder_state = [[encoder_state_h1, encoder_state_c1],[encoder_state_h2, encoder_state_c2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 decoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============decoder=============\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=FRA_VOCAB_SIZE, input_length=None, mask_zero=True)(decoder_inputs)\n",
    "lstm1 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)\n",
    "lstm2 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_dense = Dense(FRA_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "decoder_h1, _, _ = lstm1(emb_target, initial_state=encoder_state[0])\n",
    "decoder_h2, _, _ = lstm2(decoder_h1, initial_state=encoder_state[1])\n",
    "decoder_outputs = decoder_dense(decoder_h2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    7040        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 128)    9088        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  131584      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 128),  131584      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 131584      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 128),  131584      lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 71)     9159        lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 551,623\n",
      "Trainable params: 551,623\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.0216 - acc: 0.1700\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.3586 - acc: 0.1701\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 4.9042 - acc: 0.1626\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.3831 - acc: 0.1679\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.2936 - acc: 0.1684\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.2209 - acc: 0.1675\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.1641 - acc: 0.1733\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.1086 - acc: 0.1724\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.0582 - acc: 0.1693\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.0165 - acc: 0.1738\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9803 - acc: 0.1740\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9531 - acc: 0.1808\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9831 - acc: 0.1691\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 3.0609 - acc: 0.1524\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9943 - acc: 0.1584\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9471 - acc: 0.1766\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9193 - acc: 0.1800\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9071 - acc: 0.1803\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9018 - acc: 0.1915\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8937 - acc: 0.1907\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8916 - acc: 0.1803\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8795 - acc: 0.2005\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8722 - acc: 0.1763\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8481 - acc: 0.1993\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8605 - acc: 0.1756\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8241 - acc: 0.2165\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8185 - acc: 0.1875\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8281 - acc: 0.2031\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.9009 - acc: 0.1780\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8082 - acc: 0.2098\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7769 - acc: 0.2087\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8073 - acc: 0.2022\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7984 - acc: 0.2149\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7487 - acc: 0.2294\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.7366 - acc: 0.2265\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7510 - acc: 0.2240\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.8266 - acc: 0.2000\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7225 - acc: 0.2314\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7558 - acc: 0.2261\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.7195 - acc: 0.2266\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7017 - acc: 0.2370\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6962 - acc: 0.2410\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.7952 - acc: 0.2193\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.7491 - acc: 0.2312\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7015 - acc: 0.2440\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7487 - acc: 0.2373\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6573 - acc: 0.2389\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6669 - acc: 0.2479\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6411 - acc: 0.2503\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.7033 - acc: 0.2247\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6452 - acc: 0.2452\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6419 - acc: 0.2512\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6205 - acc: 0.2550\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6971 - acc: 0.2321\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6127 - acc: 0.2443\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6430 - acc: 0.2429\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6833 - acc: 0.2279\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.6692 - acc: 0.2287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6171 - acc: 0.2480\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.6073 - acc: 0.2526\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.6823 - acc: 0.2291\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5843 - acc: 0.2571\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5646 - acc: 0.2757\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5518 - acc: 0.2684\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5769 - acc: 0.2759\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.6184 - acc: 0.2556\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.6889 - acc: 0.2417\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5532 - acc: 0.2633\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5702 - acc: 0.2735\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5987 - acc: 0.2728\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6684 - acc: 0.2286\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5791 - acc: 0.2689\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5381 - acc: 0.2871\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5332 - acc: 0.2743\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5637 - acc: 0.2792\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.6569 - acc: 0.2377\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5493 - acc: 0.2738\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5481 - acc: 0.2794\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5123 - acc: 0.2924\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5573 - acc: 0.2745\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5009 - acc: 0.2798\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5392 - acc: 0.2796\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5099 - acc: 0.2738\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4984 - acc: 0.2856\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.4830 - acc: 0.2882\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5954 - acc: 0.2586\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5098 - acc: 0.2880\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4791 - acc: 0.2949\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5351 - acc: 0.2689\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.4696 - acc: 0.2856\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4953 - acc: 0.2784\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.4974 - acc: 0.2880\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.4742 - acc: 0.2875\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4518 - acc: 0.2908\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.5104 - acc: 0.2831\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.4204 - acc: 0.3187\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4574 - acc: 0.2928\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5209 - acc: 0.3017\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.5867 - acc: 0.2549\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4557 - acc: 0.2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer lstm_3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 100\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# Run training\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0)\n",
    "\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 搭建预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1,encoder_state_h2, encoder_state_c2])\n",
    "\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "decoder_h1, state_h1, state_c1 = lstm1(emb_target, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "decoder_h2, state_h2, state_c2 = lstm2(decoder_h1, initial_state=[decoder_state_input_h2, decoder_state_input_c2])\n",
    "decoder_outputs = decoder_dense(decoder_h2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2], \n",
    "                      [decoder_outputs, state_h1, state_c1, state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_input_data[1])\n",
    "print(decoder_input_data[1])\n",
    "print(decoder_output_data[1])\n",
    "print(''.join([id2en[i] for i in encoder_input_data[1]]))\n",
    "print(''.join([id2ch[i] for i in decoder_input_data[1]]))\n",
    "print(''.join([id2ch[i] for i in decoder_output_data[1]]))\n",
    "\n",
    "for k in range(50):\n",
    "    test_data = encoder_input_data[k]\n",
    "    h1, c1,h2, c2 = encoder_model.predict(test_data)\n",
    "    condition = True\n",
    "    outputs = []\n",
    "    decoder_input = [2]\n",
    "    while condition:\n",
    "        output, h1, c1, h2, c2 = decoder_model.predict([decoder_input, h1, c1, h2, c2])\n",
    "        outputs.append(np.argmax(output))\n",
    "        decoder_input = [np.argmax(output)]\n",
    "        if (np.argmax(output)) == 3 or len(outputs) > 20: condition = False\n",
    "    print(''.join([id2en[i] for i in test_data]))\n",
    "    print(''.join([id2ch[i] for i in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
