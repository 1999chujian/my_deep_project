{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I don't want it.\\t我不要.\", 'I feel relieved.\\t我感觉轻松了。', 'I get up at six.\\t我六點起床。', 'I had no choice.\\t那时我没有选择的余地。', 'I hate studying.\\t我讨厌学习。']\n",
      "英文数据:\n",
      " ['Hi.', 'Hi.', 'Run.', 'Wait!', 'Hello!', 'I try.', 'I won!', 'Oh no!', 'Cheers!', 'He ran.']\n",
      "\n",
      "中文数据:\n",
      " ['嗨。', '你好。', '你用跑的。', '等等！', '你好。', '让我来。', '我赢了。', '不会吧。', '乾杯!', '他跑了。']\n"
     ]
    }
   ],
   "source": [
    "# ========读取原始数据========\n",
    "with open('cmn.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "data = data[:1000]\n",
    "print(data[-5:])\n",
    "\n",
    "\n",
    "# 分割英文数据和中文数据\n",
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "ch_data = [line.split('\\t')[1] for line in data]\n",
    "print('英文数据:\\n', en_data[:10])\n",
    "print('\\n中文数据:\\n', ch_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "英文字典:\n",
      " {'<PAD>': 0, '<UNK>': 1, 'o': 2, 'B': 3, 'A': 4, 'j': 5, 'F': 6, 'N': 7, 'm': 8, 'g': 9, 'r': 10, 'R': 11, 'p': 12, 'O': 13, 'a': 14, 'l': 15, ' ': 16, 'w': 17, 'y': 18, \"'\": 19, 'S': 20, 'D': 21, 'U': 22, 'T': 23, 'Y': 24, 'K': 25, '!': 26, 'Q': 27, 'h': 28, 'V': 29, '?': 30, 'e': 31, 'L': 32, 'J': 33, '3': 34, 'P': 35, 'W': 36, 'd': 37, 's': 38, 'f': 39, '0': 40, 't': 41, '8': 42, 'E': 43, 'M': 44, 'u': 45, 'v': 46, 'x': 47, 'C': 48, 'q': 49, 'z': 50, '.': 51, 'k': 52, 'b': 53, '7': 54, ',': 55, 'G': 56, 'n': 57, 'I': 58, ':': 59, 'i': 60, 'c': 61, '1': 62, 'H': 63}\n",
      "\n",
      "中文字典共计\n",
      ": {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3, '粗': 4, '会': 5, '不': 6, '高': 7, '欠': 8, '允': 9, '行': 10, '科': 11, '點': 12, '嘗': 13, '给': 14, '恨': 15, '餐': 16, '認': 17, '麼': 18, '聰': 19, '浮': 20, '怨': 21, '愉': 22, '按': 23, '在': 24, '鸡': 25, '們': 26, '提': 27, '功': 28, '瘋': 29, '怕': 30, '韩': 31, '魂': 32, '緊': 33, '鎖': 34, '前': 35, '決': 36, '醉': 37, '累': 38, '她': 39, '它': 40, '樣': 41, '刚': 42, '调': 43, '相': 44, '令': 45, '斷': 46, '鸟': 47, '雾': 48, '少': 49, '设': 50, '剧': 51, '易': 52, '星': 53, '醫': 54, '愿': 55, '睡': 56, '開': 57, '往': 58, '缺': 59, '跑': 60, '耐': 61, '為': 62, '过': 63, '准': 64, '闭': 65, '職': 66, '东': 67, '臂': 68, '皺': 69, '疯': 70, '原': 71, '圈': 72, '惧': 73, '瓦': 74, '啡': 75, 'e': 76, '进': 77, '半': 78, '妻': 79, '作': 80, '賣': 81, '奇': 82, '紧': 83, '繼': 84, '插': 85, '穷': 86, '忙': 87, '鬼': 88, '？': 89, '塊': 90, '慌': 91, '明': 92, '糟': 93, '加': 94, '地': 95, '我': 96, '嗝': 97, '適': 98, '随': 99, '泳': 100, '步': 101, '茶': 102, '掃': 103, '辆': 104, '干': 105, '继': 106, '钢': 107, '裡': 108, '晚': 109, '師': 110, '來': 111, '饭': 112, '器': 113, '小': 114, '京': 115, '吃': 116, '責': 117, '菜': 118, '只': 119, '愚': 120, '何': 121, '早': 122, '月': 123, '施': 124, '帶': 125, '告': 126, '门': 127, '謊': 128, '丢': 129, '转': 130, '热': 131, '升': 132, '你': 133, '歲': 134, '脱': 135, '松': 136, '归': 137, '貪': 138, '訂': 139, '数': 140, '說': 141, '保': 142, '哭': 143, '跳': 144, '移': 145, '法': 146, '歌': 147, '逮': 148, '曲': 149, '么': 150, '就': 151, '水': 152, '箱': 153, '响': 154, '聊': 155, '留': 156, '努': 157, '们': 158, '之': 159, '7': 160, '眼': 161, '生': 162, '实': 163, '睛': 164, '慢': 165, '飞': 166, '受': 167, '定': 168, '玩': 169, '力': 170, '歡': 171, '頭': 172, '情': 173, '擔': 174, '激': 175, '要': 176, '好': 177, '期': 178, '果': 179, '拒': 180, '趴': 181, '命': 182, '計': 183, '解': 184, '辞': 185, '房': 186, '迟': 187, '货': 188, '勇': 189, '躺': 190, '备': 191, '樂': 192, '当': 193, '傻': 194, '击': 195, '人': 196, '蛋': 197, '輕': 198, '這': 199, '漂': 200, '麗': 201, '救': 202, '罪': 203, '边': 204, '衬': 205, '帮': 206, '玛': 207, '圖': 208, '找': 209, '放': 210, '工': 211, '饱': 212, '冒': 213, '嘴': 214, '付': 215, '棒': 216, '狐': 217, '溺': 218, '鄙': 219, '五': 220, '首': 221, '乐': 222, '怀': 223, '年': 224, '赢': 225, '谓': 226, '球': 227, '题': 228, '投': 229, '應': 230, '燒': 231, '谦': 232, '迷': 233, '景': 234, '声': 235, '滾': 236, '讨': 237, '塗': 238, '必': 239, '安': 240, '抗': 241, '通': 242, '現': 243, '然': 244, '怎': 245, '对': 246, '吻': 247, '学': 248, '偷': 249, '握': 250, '蠢': 251, '论': 252, '白': 253, '谢': 254, '欢': 255, '撒': 256, '魯': 257, '面': 258, '盡': 259, 't': 260, '需': 261, '識': 262, '钱': 263, '别': 264, '個': 265, '聲': 266, '失': 267, '那': 268, '幾': 269, '戒': 270, '友': 271, '愛': 272, '畏': 273, '鬆': 274, '而': 275, '十': 276, '拥': 277, '瑪': 278, '续': 279, '醒': 280, '处': 281, '联': 282, '看': 283, '1': 284, '槍': 285, '僱': 286, '尝': 287, '非': 288, '綠': 289, '妒': 290, '筆': 291, 'm': 292, '席': 293, '午': 294, '切': 295, '把': 296, '鱼': 297, '善': 298, '鲜': 299, '弱': 300, '同': 301, '下': 302, '凶': 303, '瑰': 304, '评': 305, '費': 306, '煩': 307, '孕': 308, '证': 309, '張': 310, '心': 311, '惕': 312, '拯': 313, '退': 314, '听': 315, '图': 316, '员': 317, '叫': 318, '禱': 319, '运': 320, '藏': 321, '兒': 322, '系': 323, '咖': 324, '冰': 325, '望': 326, '您': 327, '備': 328, '利': 329, '里': 330, '贺': 331, '万': 332, '經': 333, '學': 334, '駕': 335, 'o': 336, '赶': 337, '关': 338, '出': 339, '手': 340, '真': 341, 'r': 342, '埋': 343, '業': 344, '车': 345, '錢': 346, '接': 347, '脸': 348, '儿': 349, '火': 350, '话': 351, '存': 352, '旁': 353, '博': 354, '沒': 355, '气': 356, '都': 357, '醬': 358, '丟': 359, '以': 360, '英': 361, '變': 362, '運': 363, '！': 364, '置': 365, '狗': 366, '碰': 367, '搅': 368, '企': 369, '更': 370, '赏': 371, '壞': 372, '騙': 373, '须': 374, '遗': 375, '笑': 376, '當': 377, '喜': 378, '平': 379, '控': 380, '始': 381, '烟': 382, '嗨': 383, '敗': 384, '閱': 385, '飛': 386, '咳': 387, '得': 388, '見': 389, '谎': 390, '节': 391, '斯': 392, '母': 393, '份': 394, '右': 395, '目': 396, '請': 397, '憐': 398, '决': 399, '诉': 400, '麻': 401, '票': 402, '己': 403, '大': 404, '结': 405, '該': 406, 'T': 407, '也': 408, '新': 409, '響': 410, '全': 411, '察': 412, '錯': 413, '使': 414, '問': 415, '见': 416, '怪': 417, '鮮': 418, '無': 419, '盛': 420, '本': 421, '楚': 422, '信': 423, '買': 424, '谁': 425, '耳': 426, '嫉': 427, '甜': 428, '離': 429, '止': 430, '三': 431, '時': 432, '险': 433, '過': 434, '假': 435, '持': 436, '示': 437, '著': 438, '對': 439, '強': 440, '嘿': 441, '禁': 442, '抓': 443, '幫': 444, '射': 445, '哦': 446, '欺': 447, '感': 448, '难': 449, '巴': 450, '记': 451, '视': 452, '变': 453, '音': 454, '择': 455, '辦': 456, '倖': 457, '朋': 458, '伙': 459, '规': 460, '梦': 461, '習': 462, '动': 463, '尽': 464, '光': 465, '講': 466, '太': 467, '待': 468, '拉': 469, '爵': 470, '惜': 471, '胖': 472, '时': 473, '，': 474, '門': 475, '懒': 476, '和': 477, '糕': 478, '次': 479, '带': 480, '单': 481, '哪': 482, '四': 483, '呆': 484, '打': 485, '燃': 486, '佬': 487, '被': 488, '饿': 489, '圆': 490, '辱': 491, '个': 492, '冷': 493, '六': 494, '呢': 495, '报': 496, '许': 497, '蘋': 498, '跟': 499, '懂': 500, '糖': 501, '子': 502, '稍': 503, '雄': 504, '向': 505, '磁': 506, '几': 507, '世': 508, '还': 509, '種': 510, '弃': 511, '憾': 512, '。': 513, '选': 514, '會': 515, '算': 516, '啊': 517, '经': 518, '天': 519, '去': 520, '!': 521, '挥': 522, '神': 523, '波': 524, '祈': 525, '取': 526, '嗎': 527, '訴': 528, '書': 529, '雨': 530, '續': 531, '清': 532, '吸': 533, '试': 534, '成': 535, '武': 536, '余': 537, '漲': 538, '国': 539, '影': 540, '讓': 541, '家': 542, '张': 543, '他': 544, '参': 545, '念': 546, '所': 547, '闷': 548, '抵': 549, '无': 550, '哈': 551, '今': 552, '後': 553, '話': 554, '来': 555, '丽': 556, '读': 557, '羡': 558, '須': 559, '赌': 560, '虚': 561, '狸': 562, '閉': 563, '?': 564, '傷': 565, '该': 566, '節': 567, '烦': 568, '觉': 569, '书': 570, '候': 571, '動': 572, '才': 573, '味': 574, '轻': 575, '再': 576, '路': 577, '渴': 578, '魚': 579, '金': 580, '婚': 581, '士': 582, '蒼': 583, '別': 584, '沉': 585, '.': 586, '讀': 587, '为': 588, '貴': 589, '酒': 590, '比': 591, '擊': 592, '厌': 593, '此': 594, '是': 595, '铐': 596, '喊': 597, '中': 598, '欣': 599, '立': 600, '事': 601, '游': 602, '險': 603, '改': 604, '息': 605, '等': 606, '烤': 607, '助': 608, '能': 609, '祝': 610, '價': 611, '汤': 612, '电': 613, '乾': 614, '雪': 615, '聞': 616, '什': 617, '害': 618, '誤': 619, '习': 620, '困': 621, '姆': 622, '隻': 623, '独': 624, '淹': 625, '抱': 626, '洗': 627, '疲': 628, '滿': 629, '笔': 630, '飽': 631, '牙': 632, '的': 633, '車': 634, '让': 635, '幹': 636, '走': 637, '結': 638, '烧': 639, '说': 640, '羞': 641, '餓': 642, '知': 643, '男': 644, '责': 645, '牢': 646, '親': 647, '氣': 648, '着': 649, '请': 650, '方': 651, 'i': 652, '没': 653, '脚': 654, '進': 655, '语': 656, '照': 657, '举': 658, '飯': 659, '拜': 660, '慕': 661, '错': 662, '矩': 663, '常': 664, '多': 665, 'w': 666, '公': 667, '校': 668, '疼': 669, '问': 670, '活': 671, '到': 672, '材': 673, '船': 674, '快': 675, '趣': 676, '玫': 677, '由': 678, '孤': 679, '遠': 680, '贏': 681, '敢': 682, '发': 683, '默': 684, '样': 685, '岁': 686, '认': 687, '想': 688, '匙': 689, '參': 690, '条': 691, '每': 692, '拿': 693, '捕': 694, '恥': 695, '位': 696, '衣': 697, '阅': 698, '起': 699, '間': 700, '湯': 701, '了': 702, '死': 703, '木': 704, '歉': 705, '腾': 706, '搞': 707, '帳': 708, '試': 709, '亡': 710, '演': 711, '住': 712, '痛': 713, 'D': 714, ' ': 715, '秘': 716, '很': 717, '矮': 718, '鳥': 719, '弄': 720, '派': 721, '食': 722, '意': 723, '靜': 724, '3': 725, '迎': 726, '量': 727, '管': 728, '间': 729, '容': 730, '入': 731, '旗': 732, '8': 733, '压': 734, '郵': 735, '填': 736, '花': 737, '严': 738, '唱': 739, '物': 740, '兴': 741, '敬': 742, '免': 743, '諒': 744, '理': 745, '头': 746, '帝': 747, '上': 748, '危': 749, '邪': 750, '頓': 751, '匈': 752, '許': 753, '自': 754, '噢': 755, '爱': 756, '红': 757, '最': 758, '日': 759, '壯': 760, '興': 761, '忘': 762, '希': 763, '现': 764, '逝': 765, '恐': 766, '遲': 767, '胶': 768, '鑰': 769, '回': 770, '畫': 771, '美': 772, '用': 773, '铃': 774, '溜': 775, '糊': 776, '絕': 777, '性': 778, '內': 779, '画': 780, '迅': 781, '趕': 782, '難': 783, '骚': 784, '盯': 785, '托': 786, '掉': 787, '顾': 788, '离': 789, '黑': 790, '顯': 791, '恭': 792, '服': 793, '乎': 794, '任': 795, '孩': 796, '如': 797, '尊': 798, '婪': 799, '做': 800, '獨': 801, '道': 802, '眉': 803, '擦': 804, '已': 805, '降': 806, '還': 807, '这': 808, '静': 809, '有': 810, '借': 811, '确': 812, '站': 813, '吗': 814, '電': 815, '左': 816, '重': 817, '从': 818, '床': 819, '類': 820, '点': 821, '包': 822, '狂': 823, '应': 824, '消': 825, '駛': 826, '些': 827, '露': 828, '遇': 829, '一': 830, '休': 831, '台': 832, '聽': 833, '密': 834, '嗽': 835, '文': 836, '机': 837, '長': 838, 'J': 839, '酸': 840, '給': 841, '合': 842, '识': 843, '杯': 844, '亲': 845, '色': 846, '奋': 847, '倦': 848, '后': 849, '老': 850, '完': 851, '棄': 852, '送': 853, '指': 854, '坚': 855, '警': 856, '建': 857, '誰': 858, '坐': 859, '空': 860, '吧': 861, '停': 862, '身': 863, '病': 864, '客': 865, '父': 866, '正': 867, '速': 868, '可': 869, '乡': 870, '开': 871}\n"
     ]
    }
   ],
   "source": [
    "# 特殊字符\n",
    "SOURCE_CODES = ['<PAD>', '<UNK>']\n",
    "TARGET_CODES = ['<PAD>', '<EOS>', '<UNK>', '<GO>']  # 在target中，需要增加<GO>与<EOS>特殊字符\n",
    "\n",
    "# 分别生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = SOURCE_CODES + list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "# 分别生成中英文字典\n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = TARGET_CODES + list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "print('\\n英文字典:\\n', en2id)\n",
    "print('\\n中文字典共计\\n:', ch2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char: Hi.\n",
      "index: [63, 60, 51]\n"
     ]
    }
   ],
   "source": [
    "# 利用字典，映射数据\n",
    "en_num_data = [[en2id[en] for en in line] for line in en_data]\n",
    "ch_num_data = [[ch2id['<GO>']] + [ch2id[ch] for ch in line] + [ch2id['<EOS>']] for line in ch_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line] + [ch2id['<EOS>']] for line in ch_data]\n",
    "\n",
    "print('char:', en_data[1])\n",
    "print('index:', en_num_data[1])\n",
    "\n",
    "en_maxlength = max([len(line) for line in en_num_data])\n",
    "ch_maxlength = max([len(line) for line in ch_num_data])\n",
    "\n",
    "# 文本数据转化为数字数据\n",
    "en_num_data = [data + [en2id['<PAD>']] * (en_maxlength - len(data)) for data in en_num_data]\n",
    "ch_num_data = [data + [en2id['<PAD>']] * (ch_maxlength - len(data)) for data in ch_num_data]\n",
    "de_num_data = [data + [en2id['<PAD>']] * (ch_maxlength - len(data)) for data in de_num_data]\n",
    "\n",
    "\n",
    "# 设计数据生成器\n",
    "def batch_data(en_num_data, ch_num_data, de_num_data, batch_size):\n",
    "    batch_num = len(en_num_data) // batch_size\n",
    "    for i in range(batch_num):\n",
    "        begin = i * batch_size\n",
    "        end = begin + batch_size\n",
    "        x = en_num_data[begin:end]\n",
    "        y = ch_num_data[begin:end]\n",
    "        z = de_num_data[begin:end]\n",
    "        yield x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_encoder_seq_length = en_maxlength\n",
    "max_decoder_seq_length = ch_maxlength\n",
    "keepprb = 0.9\n",
    "\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "\n",
    "HIDDEN_LAYERS = 2\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "learning_rate = 0.003\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "BATCH_NUMS = len(ch_num_data) // BATCH_SIZE\n",
    "MAX_GRAD_NORM = 1\n",
    "\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "Once retrieved, the word embeddings are then fed as input into the main network, which consists of two multi-layer RNNs – an encoder for the source language and a decoder for the target language. These two RNNs, in principle, can share the same weights; however, in practice, we often use two different RNN parameters (such models do a better job when fitting large training datasets). The encoder RNN uses zero vectors as its starting states and is built as follows:\n",
    "```py\n",
    "# Build RNN cell\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "# Run Dynamic RNN\n",
    "#   encoder_outputs: [max_time, batch_size, num_units]\n",
    "#   encoder_state: [batch_size, num_units]\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp,\n",
    "    sequence_length=source_sequence_length, time_major=True)\n",
    "```\n",
    "Note that sentences have different lengths to avoid wasting computation, we tell dynamic_rnn the exact source sentence lengths through source_sequence_length. Since our input is time major, we set time_major=True. Here, we build only a single layer LSTM, encoder_cell. We will describe how to build multi-layer LSTMs, add dropout, and use attention in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs = tf.placeholder(tf.int32, [BATCH_SIZE, max_encoder_seq_length])\n",
    "decoder_inputs = tf.placeholder(tf.int32, [BATCH_SIZE, max_decoder_seq_length])\n",
    "targets = tf.placeholder(tf.int32, [BATCH_SIZE, max_decoder_seq_length])\n",
    "keepprb = tf.placeholder(tf.float32)\n",
    "\n",
    "# # Embedding\n",
    "with tf.name_scope('embedding_encoder'):\n",
    "\tencoder_embedding = tf.get_variable('embedding_encoder', [EN_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "\tencoder_emb = tf.nn.embedding_lookup(encoder_embedding, encoder_inputs)\n",
    "\tencoder_emb = tf.nn.dropout(encoder_emb, keepprb)\n",
    "\n",
    "\n",
    "# encoder\n",
    "with tf.variable_scope('encoder'):\n",
    "\tencoder_lstm = tf.contrib.rnn.LSTMCell(HIDDEN_SIZE, state_is_tuple=True)\n",
    "\tencoder_lstm = tf.contrib.rnn.DropoutWrapper(encoder_lstm, output_keep_prob=keepprb)\n",
    "\tencoder_cell = tf.contrib.rnn.MultiRNNCell([encoder_lstm] * HIDDEN_LAYERS)\n",
    "\tinitial_state = encoder_cell.zero_state(BATCH_SIZE, tf.float32)\n",
    "\t_, final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb, initial_state=initial_state)\n",
    "\n",
    "\n",
    "with tf.name_scope('embedding_decoder'):\n",
    "\tdecoder_embedding = tf.get_variable('embedding_decoder', [CH_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "\tdecoder_emb = tf.nn.embedding_lookup(decoder_embedding, decoder_inputs)\n",
    "\tdecoder_emb = tf.nn.dropout(decoder_emb, keepprb)\n",
    "\n",
    "\n",
    "# decoder\n",
    "with tf.variable_scope('decoder'):\n",
    "\tdecoder_lstm = tf.contrib.rnn.LSTMCell(HIDDEN_SIZE, state_is_tuple=True)\n",
    "\tdecoder_lstm = tf.contrib.rnn.DropoutWrapper(decoder_lstm, output_keep_prob=keepprb)\n",
    "\tdecoder_cell = tf.contrib.rnn.MultiRNNCell([decoder_lstm] * HIDDEN_LAYERS)\n",
    "\toutputs, _ = tf.nn.dynamic_rnn(decoder_cell, decoder_emb, initial_state=final_state)\n",
    "\toutputs = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    w = tf.get_variable('outputs_weight', [HIDDEN_SIZE, CH_VOCAB_SIZE])\n",
    "    b = tf.get_variable('outputs_bias', [CH_VOCAB_SIZE])\n",
    "    logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "with tf.variable_scope('optimizer'):\n",
    "    # ======计算损失=======\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(targets, [-1])], \n",
    "                                                            [tf.ones([BATCH_SIZE * max_decoder_seq_length], dtype=tf.float32)])\n",
    "    cost = tf.reduce_sum(loss) / BATCH_SIZE\n",
    "\n",
    "    # =============优化算法==============\n",
    "    # =============学习率衰减==============\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step, BATCH_NUMS, 0.99, staircase=True)\n",
    "\n",
    "                # =======通过clip_by_global_norm()控制梯度大小======\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, trainable_variables), MAX_GRAD_NORM)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "\t\t# ==============预测输出=============\n",
    "predict = tf.reshape(tf.argmax(logits, 1), [-1, max_decoder_seq_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 1 iter: 50 cost: 35.23863157934072\n",
      "text: 我是是了。\n",
      "label: 开车慢点。\n",
      "epochs: 1 iter: 100 cost: 33.481122854984164\n",
      "text: 我我我。。\n",
      "label: 我很快乐。\n",
      "epochs: 2 iter: 50 cost: 24.81053655974719\n",
      "text: 我是了。。\n",
      "label: 开车慢点。\n",
      "epochs: 2 iter: 100 cost: 26.30845823191633\n",
      "text: 我是。。。\n",
      "label: 我很快乐。\n",
      "epochs: 3 iter: 50 cost: 23.30006760967021\n",
      "text: 我了。。。\n",
      "label: 开车慢点。\n",
      "epochs: 3 iter: 100 cost: 25.140980681987724\n",
      "text: 我是。。。\n",
      "label: 我很快乐。\n",
      "epochs: 4 iter: 50 cost: 22.159257246523488\n",
      "text: 我是。。。\n",
      "label: 开车慢点。\n",
      "epochs: 4 iter: 100 cost: 23.647061097501503\n",
      "text: 我是一。。\n",
      "label: 我很快乐。\n",
      "epochs: 5 iter: 50 cost: 21.20319825775769\n",
      "text: 我疼。。。\n",
      "label: 开车慢点。\n",
      "epochs: 5 iter: 100 cost: 22.643292398163766\n",
      "text: 我是快。。\n",
      "label: 我很快乐。\n",
      "epochs: 6 iter: 50 cost: 20.411330495561874\n",
      "text: 我心。？。\n",
      "label: 开车慢点。\n",
      "epochs: 6 iter: 100 cost: 21.806272525980017\n",
      "text: 我們大樂。\n",
      "label: 我很快乐。\n",
      "epochs: 7 iter: 50 cost: 19.590290633999572\n",
      "text: 我，啊？。\n",
      "label: 开车慢点。\n",
      "epochs: 7 iter: 100 cost: 20.674438303167168\n",
      "text: 我是快。。\n",
      "label: 我很快乐。\n",
      "epochs: 8 iter: 50 cost: 18.4663860165343\n",
      "text: 我，嗎。。\n",
      "label: 开车慢点。\n",
      "epochs: 8 iter: 100 cost: 19.514611003374814\n",
      "text: 我是谎樂。\n",
      "label: 我很快乐。\n",
      "epochs: 9 iter: 50 cost: 17.456917762756348\n",
      "text: 你心好！。\n",
      "label: 开车慢点。\n",
      "epochs: 9 iter: 100 cost: 18.268375984346022\n",
      "text: 我是快中。\n",
      "label: 我很快乐。\n",
      "epochs: 10 iter: 50 cost: 16.60320046483254\n",
      "text: 我好机！。\n",
      "label: 开车慢点。\n",
      "epochs: 10 iter: 100 cost: 17.321683055222636\n",
      "text: 我們厌服。\n",
      "label: 我很快乐。\n",
      "epochs: 11 iter: 50 cost: 15.830340443825236\n",
      "text: 别好啊！。\n",
      "label: 开车慢点。\n",
      "epochs: 11 iter: 100 cost: 16.57442157437103\n",
      "text: 我不快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 12 iter: 50 cost: 15.10644311321025\n",
      "text: 你心好！。\n",
      "label: 开车慢点。\n",
      "epochs: 12 iter: 100 cost: 15.633184635277951\n",
      "text: 我今快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 13 iter: 50 cost: 14.397868584613411\n",
      "text: 你烦！！。\n",
      "label: 开车慢点。\n",
      "epochs: 13 iter: 100 cost: 14.695532095552695\n",
      "text: 我不备樂。\n",
      "label: 我很快乐。\n",
      "epochs: 14 iter: 50 cost: 13.285928940286441\n",
      "text: 你一机！。\n",
      "label: 开车慢点。\n",
      "epochs: 14 iter: 100 cost: 13.60535310976433\n",
      "text: 我喜激玩。\n",
      "label: 我很快乐。\n",
      "epochs: 15 iter: 50 cost: 12.811808060626594\n",
      "text: 你好点点。\n",
      "label: 开车慢点。\n",
      "epochs: 15 iter: 100 cost: 13.130466490080863\n",
      "text: 我喜激樂。\n",
      "label: 我很快乐。\n",
      "epochs: 16 iter: 50 cost: 12.212671056085703\n",
      "text: 這，慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 16 iter: 100 cost: 12.265686044789325\n",
      "text: 我不快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 17 iter: 50 cost: 11.7290989233523\n",
      "text: 你一慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 17 iter: 100 cost: 11.745327366722954\n",
      "text: 我脱快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 18 iter: 50 cost: 11.001017998675911\n",
      "text: 别！机点。\n",
      "label: 开车慢点。\n",
      "epochs: 18 iter: 100 cost: 10.934184141833374\n",
      "text: 我应快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 19 iter: 50 cost: 10.673479274827606\n",
      "text: 汤车慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 19 iter: 100 cost: 10.622632272315748\n",
      "text: 我今快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 20 iter: 50 cost: 9.973520424901222\n",
      "text: 别奇門点。\n",
      "label: 开车慢点。\n",
      "epochs: 20 iter: 100 cost: 9.802953845322735\n",
      "text: 我是激乐。\n",
      "label: 我很快乐。\n",
      "epochs: 21 iter: 50 cost: 9.432231494358607\n",
      "text: 你车慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 21 iter: 100 cost: 9.380899414871678\n",
      "text: 我喜快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 22 iter: 50 cost: 9.091580176840024\n",
      "text: 你奇慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 22 iter: 100 cost: 9.052969219708684\n",
      "text: 我很快乐。\n",
      "label: 我很快乐。\n",
      "epochs: 23 iter: 50 cost: 8.909060108418366\n",
      "text: 你车慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 23 iter: 100 cost: 8.861469716736764\n",
      "text: 我是快乐。\n",
      "label: 我很快乐。\n",
      "epochs: 24 iter: 50 cost: 8.687313615059367\n",
      "text: 你畫慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 24 iter: 100 cost: 8.616968501697887\n",
      "text: 我应快乐。\n",
      "label: 我很快乐。\n",
      "epochs: 25 iter: 50 cost: 8.56652205330985\n",
      "text: 她车慢点。\n",
      "label: 开车慢点。\n",
      "epochs: 25 iter: 100 cost: 8.5950846527562\n",
      "text: 我是快樂。\n",
      "label: 我很快乐。\n",
      "epochs: 26 iter: 50 cost: 9.26077717177722\n",
      "text: 汤，托点。\n",
      "label: 开车慢点。\n",
      "epochs: 26 iter: 100 cost: 9.006554454264014\n",
      "text: 我喜快樂。\n",
      "label: 我很快乐。\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "\twriter = tf.summary.FileWriter('logs/tensorboard', tf.get_default_graph())\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tfor k in range(EPOCHS):\n",
    "\t\ttotal_loss = 0.\n",
    "\t\tdata_generator = batch_data(en_num_data, ch_num_data, de_num_data, BATCH_SIZE)\n",
    "\t\tfor i in range(BATCH_NUMS):\n",
    "\t\t\ten_batch, ch_batch, de_batch = next(data_generator)\n",
    "\t\t\tfeed = {encoder_inputs: en_batch, decoder_inputs: ch_batch, targets: de_batch, keepprb: 0.8}\n",
    "\t\t\tcosts, _ = sess.run([cost, opt], feed_dict=feed)\n",
    "\t\t\ttotal_loss += costs\n",
    "\t\t\tif (i+1) % 50 == 0:\n",
    "\t\t\t\tprint('epochs:', k + 1, 'iter:', i + 1, 'cost:', total_loss / i + 1)\n",
    "\t\t\t\t#print('predict:', sess.run(predict[0], feed_dict=feed))\n",
    "\t\t\t\tprint('text:', ''.join([id2ch[i] for i in sess.run(predict[0], feed_dict=feed) if(i != 0 and i != 1)]))\n",
    "\t\t\t\tprint('label:', ''.join([id2ch[i] for i in de_batch[0] if(i != 0 and i != 1)]))\n",
    "                \n",
    "\tsaver.save(sess, './checkpoints/lstm.ckpt')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
