{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于seq2seq的中英文翻译系统\n",
    "## 1. 项目背景\n",
    "之前我们利用lstm进行建模，设计了一个自动生成莫言小说的模型，这次想要利用rnn的特点搭建一个中英文的翻译系统。传统的RNN输入和输出长度要一致，而seq2seq在RNN的基础上进行改进，实现了变长序列的输入和输出，广泛的应用在了机器翻译、对话系统、文本摘要等领域。 \n",
    "- 代码参考：https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "## 2. 项目数据\n",
    "项目数据使用英法文翻译数据集，来实现字符级的seq2seq模型的训练。 \n",
    "该文件来自于:http://www.manythings.org/anki/\n",
    "\n",
    "内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Try this.\\tEssayez ceci !']\n"
     ]
    }
   ],
   "source": [
    "# ========读取原始数据========\n",
    "with open('fra.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "data = data[:400]\n",
    "print(data[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据处理\n",
    "### 3.1 生成字典\n",
    "我们需要将汉字和英文映射为能够输入到模型中的数字信息，就需要建立一个映射关系，需要生成汉字和数字互相映射的字典。\n",
    "- 我们将英文按照每个字母对应一个id\n",
    "- 我们将中文按照每一个汉字对应一个id\n",
    "- **注意增加：**\n",
    "    1. 未知符号：UNK\n",
    "    2. 补齐符号：PAD\n",
    "    3. 开始符号：GO\n",
    "    4. 结束符号：EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文数据: ['Go.', 'Run!', 'Run!', 'Fire!', 'Help!', 'Jump.', 'Stop!', 'Stop!', 'Stop!', 'Wait!']\n",
      "中文数据: ['Va !', 'Cours\\u202f!', 'Courez\\u202f!', 'Au feu !', \"À l'aide\\u202f!\", 'Saute.', 'Ça suffit\\u202f!', 'Stop\\u202f!', 'Arrête-toi !', 'Attends !']\n",
      "英文字典:\n",
      " {'__PAD__': 0, '__UNK__': 1, ' ': 2, 'k': 3, 'M': 4, 'H': 5, 's': 6, 'w': 7, 'p': 8, 'c': 9, ',': 10, 'Y': 11, 't': 12, 'h': 13, 'v': 14, 'D': 15, 'G': 16, 'j': 17, 'l': 18, '.': 19, 'R': 20, \"'\": 21, 'o': 22, 'J': 23, 'P': 24, 'b': 25, 'T': 26, 'y': 27, 'K': 28, 'O': 29, 'W': 30, 'N': 31, 'L': 32, 'u': 33, 'e': 34, 'S': 35, 'g': 36, '?': 37, 'f': 38, 'a': 39, 'B': 40, 'C': 41, 'm': 42, 'A': 43, '9': 44, 'z': 45, 'q': 46, 'i': 47, 'n': 48, 'r': 49, '1': 50, 'd': 51, '!': 52, 'F': 53, 'I': 54}\n",
      "法文字典\n",
      ": {'__PAD__': 0, '__UNK__': 1, '__GO__': 2, '__EOS__': 3, ' ': 4, 'V': 5, 'à': 6, 'x': 7, 'E': 8, 'M': 9, 'H': 10, 's': 11, 'p': 12, 'c': 13, ',': 14, 't': 15, 'h': 16, 'Q': 17, 'v': 18, 'D': 19, 'G': 20, 'j': 21, 'î': 22, '\\u2009': 23, 'ê': 24, 'l': 25, '.': 26, 'é': 27, \"'\": 28, 'R': 29, 'o': 30, 'J': 31, 'P': 32, 'b': 33, 'T': 34, 'y': 35, 'O': 36, 'N': 37, 'À': 38, 'L': 39, '-': 40, 'Ç': 41, '’': 42, 'ç': 43, 'u': 44, 'e': 45, 'S': 46, 'g': 47, 'f': 48, '?': 49, 'è': 50, 'û': 51, 'a': 52, 'B': 53, 'C': 54, 'm': 55, 'A': 56, 'z': 57, '9': 58, 'q': 59, 'i': 60, 'â': 61, 'r': 62, 'n': 63, '\\u202f': 64, '1': 65, 'É': 66, 'd': 67, '!': 68, 'F': 69, 'I': 70}\n"
     ]
    }
   ],
   "source": [
    "# 分割英文数据和中文数据\n",
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "fra_data = [line.split('\\t')[1] for line in data]\n",
    "print('英文数据:', en_data[:10])\n",
    "print('中文数据:', fra_data[:10])\n",
    "\n",
    "# 分别生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = ['__PAD__', '__UNK__'] + list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "fra_vocab = set(''.join(fra_data))\n",
    "id2fra = ['__PAD__', '__UNK__', '__GO__', '__EOS__'] + list(fra_vocab)\n",
    "fra2id = {c:i for i,c in enumerate(id2fra)}\n",
    "\n",
    "print('英文字典:\\n', en2id)\n",
    "print('法文字典\\n:', fra2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 转换输入数据格式\n",
    "建立字典后，将文本数据映射为数字数据形式，并整理为矩阵格式。在生成之前需要考虑训练该模型所需的数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 22, 19], [20, 33, 48, 52], [20, 33, 48, 52], [53, 47, 49, 34, 52], [5, 34, 18, 8, 52]]\n",
      "[[2, 5, 52, 4, 68], [2, 54, 30, 44, 62, 11, 64, 68], [2, 54, 30, 44, 62, 45, 57, 64, 68], [2, 56, 44, 4, 48, 45, 44, 4, 68], [2, 38, 4, 25, 28, 52, 60, 67, 45, 64, 68]]\n",
      "[[5, 52, 4, 68, 3], [54, 30, 44, 62, 11, 64, 68, 3], [54, 30, 44, 62, 45, 57, 64, 68, 3], [56, 44, 4, 48, 45, 44, 4, 68, 3], [38, 4, 25, 28, 52, 60, 67, 45, 64, 68, 3]]\n"
     ]
    }
   ],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "fra_num_data = [[fra2id['__GO__']] + [fra2id[ch] for ch in line] for line in fra_data]\n",
    "de_num_data = [[fra2id[fra] for fra in line] + [fra2id['__EOS__']] for line in fra_data]\n",
    "print(en_num_data[:5])\n",
    "print(fra_num_data[:5])\n",
    "print(de_num_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 整理训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "31\n",
      "(400, 31, 71)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in fra_num_data])\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)\n",
    "\n",
    "\n",
    "encoder_input_data = [line + [0] * (max_encoder_seq_length-len(line)) for line in en_num_data]\n",
    "decoder_input_data = [line + [0] * (max_decoder_seq_length-len(line)) for line in fra_num_data]\n",
    "decoder_output_data = [line + [0] * (max_decoder_seq_length-len(line)) for line in de_num_data]\n",
    "decoder_target_data = np.zeros((len(fra_num_data), max_decoder_seq_length, len(fra2id)), dtype='float32')\n",
    "for i in range(len(fra_num_data)):\n",
    "    for j in range(max_decoder_seq_length):\n",
    "        decoder_target_data[i,j,decoder_output_data[i][j]] = 1\n",
    "\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======预定义模型参数========\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "FRA_VOCAB_SIZE = len(fra2id)\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择与建模\n",
    "### 4.1 encoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ======================================keras model==================================\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Embedding\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE, input_length=None, mask_zero=True)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)(emb_inp)\n",
    "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, activation='relu', return_state=True, dropout=0.2)(encoder_h1)\n",
    "encoder_state = [[encoder_state_h1, encoder_state_c1],[encoder_state_h2, encoder_state_c2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 decoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============decoder=============\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=FRA_VOCAB_SIZE, input_length=None, mask_zero=True)(decoder_inputs)\n",
    "lstm1 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)\n",
    "lstm2 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_dense = Dense(FRA_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "decoder_h1, _, _ = lstm1(emb_target, initial_state=encoder_state[0])\n",
    "decoder_h2, _, _ = lstm2(decoder_h1, initial_state=encoder_state[1])\n",
    "decoder_outputs = decoder_dense(decoder_h2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    7040        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 128)    9088        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  131584      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 128),  131584      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 131584      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 128),  131584      lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 71)     9159        lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 551,623\n",
      "Trainable params: 551,623\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 2.4124 - acc: 0.3177\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.4665 - acc: 0.3028\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.3824 - acc: 0.3282\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.3811 - acc: 0.3156\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3672 - acc: 0.3217\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3384 - acc: 0.3371\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.3274 - acc: 0.3456\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.3244 - acc: 0.3485\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3078 - acc: 0.3499\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.2854 - acc: 0.3605\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.2802 - acc: 0.3573\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2610 - acc: 0.3584\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.2404 - acc: 0.3647\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.2317 - acc: 0.3701\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.2156 - acc: 0.3757\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.2023 - acc: 0.3840\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.1876 - acc: 0.3873\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.1759 - acc: 0.3819\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.1553 - acc: 0.3885\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.1463 - acc: 0.3885\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.1178 - acc: 0.3984\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.1153 - acc: 0.4027\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.1031 - acc: 0.4066\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.0907 - acc: 0.4054\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.0735 - acc: 0.4143\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.0674 - acc: 0.4115\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.0413 - acc: 0.4194\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.0280 - acc: 0.4208\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.0152 - acc: 0.4215\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 2.0038 - acc: 0.4315\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.9860 - acc: 0.4333\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.9728 - acc: 0.4394\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.9598 - acc: 0.4341\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.9386 - acc: 0.4408\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.9388 - acc: 0.4357\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.9204 - acc: 0.4455\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.9002 - acc: 0.4445\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.8867 - acc: 0.4464\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.8787 - acc: 0.4468\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.8677 - acc: 0.4524\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.8459 - acc: 0.4583\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.8407 - acc: 0.4633\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.8258 - acc: 0.4625\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.8133 - acc: 0.4640\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.7992 - acc: 0.4662\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.7879 - acc: 0.4783\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.7800 - acc: 0.4738\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.7661 - acc: 0.4754\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.7664 - acc: 0.4769\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.7240 - acc: 0.4825\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.7302 - acc: 0.4892\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.7133 - acc: 0.4903\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.7024 - acc: 0.4903\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.6868 - acc: 0.5010\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.6798 - acc: 0.4964\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.6679 - acc: 0.5031\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.6481 - acc: 0.5061\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.6440 - acc: 0.5045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.6309 - acc: 0.5085\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.6218 - acc: 0.5162\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.6022 - acc: 0.5201\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5980 - acc: 0.5271\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5764 - acc: 0.5259\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.5765 - acc: 0.5253\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5749 - acc: 0.5264\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5600 - acc: 0.5324\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5470 - acc: 0.5329\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.5432 - acc: 0.5394\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.5190 - acc: 0.5431\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.5104 - acc: 0.5411\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5053 - acc: 0.5464\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.5005 - acc: 0.5417\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5045 - acc: 0.5427\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.4625 - acc: 0.5610\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.4699 - acc: 0.5608\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.4752 - acc: 0.5555\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.4420 - acc: 0.5683\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.4302 - acc: 0.5706\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.4137 - acc: 0.5704\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.4085 - acc: 0.5731\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3982 - acc: 0.5755\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3835 - acc: 0.5787\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3701 - acc: 0.5850\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3744 - acc: 0.5831\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3661 - acc: 0.5880\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3603 - acc: 0.5876\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3311 - acc: 0.5902\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3253 - acc: 0.5952\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3158 - acc: 0.5966\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3078 - acc: 0.6069\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.3054 - acc: 0.6053\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.2887 - acc: 0.6160\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2683 - acc: 0.6160\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.2779 - acc: 0.6074\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.2644 - acc: 0.6143\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.2514 - acc: 0.6208\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2312 - acc: 0.6269\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2284 - acc: 0.6234\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.2318 - acc: 0.6260\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 1.2046 - acc: 0.6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer lstm_3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 100\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# Run training\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0)\n",
    "\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 搭建预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1,encoder_state_h2, encoder_state_c2])\n",
    "\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "decoder_h1, state_h1, state_c1 = lstm1(emb_target, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "decoder_h2, state_h2, state_c2 = lstm2(decoder_h1, initial_state=[decoder_state_input_h2, decoder_state_input_c2])\n",
    "decoder_outputs = decoder_dense(decoder_h2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2], \n",
    "                      [decoder_outputs, state_h1, state_c1, state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 33, 48, 52, 0, 0, 0, 0, 0]\n",
      "[2, 54, 30, 44, 62, 11, 64, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[54, 30, 44, 62, 11, 64, 68, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Run!__PAD____PAD____PAD____PAD____PAD__\n",
      "__GO__Cours !__PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD__\n",
      "Cours !__EOS____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD____PAD__\n",
      "0 th:\n",
      "Go.\n",
      "e tatte !\n",
      "1 th:\n",
      "Run!\n",
      "ettte !\n",
      "2 th:\n",
      "Run!\n",
      "ettte !\n",
      "3 th:\n",
      "Fire!\n",
      "ettte !\n",
      "4 th:\n",
      "Help!\n",
      "e tatte !\n",
      "5 th:\n",
      "Jump.\n",
      "ett paui !\n",
      "6 th:\n",
      "Stop!\n",
      "ettte !\n",
      "7 th:\n",
      "Stop!\n",
      "ettte !\n",
      "8 th:\n",
      "Stop!\n",
      "ettte !\n",
      "9 th:\n",
      "Wait!\n",
      "ett paui !\n",
      "10 th:\n",
      "Wait!\n",
      "ett paui !\n",
      "11 th:\n",
      "Go on.\n",
      "e tatte !\n",
      "12 th:\n",
      "Go on.\n",
      "e tatte !\n",
      "13 th:\n",
      "Go on.\n",
      "e tatte !\n",
      "14 th:\n",
      "I see.\n",
      "atte !\n",
      "15 th:\n",
      "I try.\n",
      "atte !\n",
      "16 th:\n",
      "I won!\n",
      "atte !\n",
      "17 th:\n",
      "I won!\n",
      "atte !\n",
      "18 th:\n",
      "Oh no!\n",
      "ett paui !\n",
      "19 th:\n",
      "Attack!\n",
      "ettte !\n",
      "20 th:\n",
      "Attack!\n",
      "ettte !\n",
      "21 th:\n",
      "Cheers!\n",
      "ettte !\n",
      "22 th:\n",
      "Cheers!\n",
      "ettte !\n",
      "23 th:\n",
      "Cheers!\n",
      "ettte !\n",
      "24 th:\n",
      "Cheers!\n",
      "ettte !\n",
      "25 th:\n",
      "Get up.\n",
      "e tatte !\n",
      "26 th:\n",
      "Go now.\n",
      "e tatte !\n",
      "27 th:\n",
      "Go now.\n",
      "e tatte !\n",
      "28 th:\n",
      "Go now.\n",
      "e tatte !\n",
      "29 th:\n",
      "Got it!\n",
      "e tatte !\n",
      "30 th:\n",
      "Got it!\n",
      "e tatte !\n",
      "31 th:\n",
      "Got it?\n",
      "e tatte !\n",
      "32 th:\n",
      "Got it?\n",
      "e tatte !\n",
      "33 th:\n",
      "Got it?\n",
      "e tatte !\n",
      "34 th:\n",
      "Hop in.\n",
      "e tatte !\n",
      "35 th:\n",
      "Hop in.\n",
      "e tatte !\n",
      "36 th:\n",
      "Hug me.\n",
      "e tatte !\n",
      "37 th:\n",
      "Hug me.\n",
      "e tatte !\n",
      "38 th:\n",
      "I fell.\n",
      "atte !\n",
      "39 th:\n",
      "I fell.\n",
      "atte !\n",
      "40 th:\n",
      "I know.\n",
      "atte !\n",
      "41 th:\n",
      "I left.\n",
      "atte !\n",
      "42 th:\n",
      "I left.\n",
      "atte !\n",
      "43 th:\n",
      "I lost.\n",
      "atte !\n",
      "44 th:\n",
      "I'm 19.\n",
      "atte !\n",
      "45 th:\n",
      "I'm OK.\n",
      "atte !\n",
      "46 th:\n",
      "I'm OK.\n",
      "atte !\n",
      "47 th:\n",
      "Listen.\n",
      "ettte !\n",
      "48 th:\n",
      "No way!\n",
      "ett paui !\n",
      "49 th:\n",
      "No way!\n",
      "ett paui !\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data[1])\n",
    "print(decoder_input_data[1])\n",
    "print(decoder_output_data[1])\n",
    "print(''.join([id2en[i] for i in encoder_input_data[1]]))\n",
    "print(''.join([id2fra[i] for i in decoder_input_data[1]]))\n",
    "print(''.join([id2fra[i] for i in decoder_output_data[1]]))\n",
    "\n",
    "for k in range(50):\n",
    "    test_data = encoder_input_data[k]\n",
    "    h1, c1,h2, c2 = encoder_model.predict(test_data)\n",
    "    condition = True\n",
    "    outputs = []\n",
    "    decoder_input = [2]\n",
    "    while condition:\n",
    "        output, h1, c1, h2, c2 = decoder_model.predict([decoder_input, h1, c1, h2, c2])\n",
    "        decoder_input = [np.argmax(output)]\n",
    "        if (np.argmax(output)) == 3 or len(outputs) > 20: condition = False\n",
    "        outputs.append(np.argmax(output))\n",
    "    print(k,'th:')\n",
    "    print(''.join([id2en[i] for i in test_data if i != 0]))\n",
    "    print(''.join([id2fra[i] for i in outputs  if i != 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
