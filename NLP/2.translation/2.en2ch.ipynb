{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于seq2seq的中英文翻译系统\n",
    "## 1. 项目背景\n",
    "之前我们利用lstm进行建模，设计了一个自动生成莫言小说的模型，这次想要利用rnn的特点搭建一个中英文的翻译系统。传统的RNN输入和输出长度要一致，而seq2seq在RNN的基础上进行改进，实现了变长序列的输入和输出，广泛的应用在了机器翻译、对话系统、文本摘要等领域。 \n",
    "- 代码参考：https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "## 2. 项目数据\n",
    "项目数据使用中英文翻译数据集，来实现字符级的seq2seq模型的训练。 \n",
    "该文件来自于:http://www.manythings.org/anki/\n",
    "\n",
    "内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Try hard.\\t努力。']\n"
     ]
    }
   ],
   "source": [
    "# ========读取原始数据========\n",
    "with open('cmn.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "data = data[:100]\n",
    "print(data[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据处理\n",
    "### 3.1 生成字典\n",
    "我们需要将汉字和英文映射为能够输入到模型中的数字信息，就需要建立一个映射关系，需要生成汉字和数字互相映射的字典。\n",
    "- 我们将英文按照每个字母对应一个id\n",
    "- 我们将中文按照每一个汉字对应一个id\n",
    "- **注意增加：**\n",
    "    1. 未知符号：UNK\n",
    "    2. 补齐符号：PAD\n",
    "    3. 开始符号：GO\n",
    "    4. 结束符号：EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文数据: ['Hi.', 'Hi.', 'Run.', 'Wait!', 'Hello!', 'I try.', 'I won!', 'Oh no!', 'Cheers!', 'He ran.']\n",
      "中文数据: ['\\t嗨。\\n', '\\t你好。\\n', '\\t你用跑的。\\n', '\\t等等！\\n', '\\t你好。\\n', '\\t让我来。\\n', '\\t我赢了。\\n', '\\t不会吧。\\n', '\\t乾杯!\\n', '\\t他跑了。\\n']\n",
      "\t\n",
      "嗨\n",
      "。\n",
      "\n",
      "\n",
      "英文字典:\n",
      " {'P': 0, 'n': 1, 'f': 2, 'g': 3, \"'\": 4, 'k': 5, 's': 6, 'I': 7, 'i': 8, 'h': 9, 'Y': 10, 'L': 11, 'N': 12, 'q': 13, 'r': 14, 'K': 15, 'D': 16, 'w': 17, 'W': 18, 'c': 19, '.': 20, 'T': 21, '?': 22, 'R': 23, ' ': 24, 'e': 25, 'm': 26, 'd': 27, 'b': 28, 'S': 29, 't': 30, 'O': 31, 'J': 32, 'v': 33, 'a': 34, 'u': 35, 'p': 36, 'C': 37, 'H': 38, 'o': 39, 'B': 40, '!': 41, 'A': 42, 'G': 43, 'l': 44, 'y': 45}\n",
      "中文字典共计:\n",
      " {'用': 0, '持': 1, '辞': 2, '往': 3, '出': 4, '试': 5, '起': 6, '抱': 7, '会': 8, '弃': 9, '们': 10, '忘': 11, '始': 12, '。': 13, '力': 14, '\\t': 15, '气': 16, '来': 17, '付': 18, '开': 19, '前': 20, '嗨': 21, '很': 22, '\\n': 23, '開': 24, '生': 25, '病': 26, '欢': 27, '呆': 28, '立': 29, '游': 30, '我': 31, '了': 32, '不': 33, '!': 34, '门': 35, '为': 36, '你': 37, '让': 38, '他': 39, '可': 40, '得': 41, '上': 42, '失': 43, '完': 44, '洗': 45, '动': 46, '嘴': 47, '清': 48, '她': 49, '点': 50, '告': 51, '关': 52, '再': 53, '快': 54, '杯': 55, '跑': 56, '坚': 57, '事': 58, '么': 59, '随': 60, '公': 61, '好': 62, '静': 63, '帮': 64, '联': 65, '一': 66, '后': 67, '錢': 68, '定': 69, '是': 70, '乾': 71, '冷': 72, '意': 73, '别': 74, '道': 75, '走': 76, '系': 77, '平': 78, '什': 79, '它': 80, '滾': 81, '抓': 82, '管': 83, '进': 84, '放': 85, '迎': 86, '泳': 87, '們': 88, '吧': 89, '吻': 90, '退': 91, '？': 92, '同': 93, '，': 94, '趕': 95, '美': 96, '听': 97, '赢': 98, '个': 99, '和': 100, '铐': 101, '住': 102, '世': 103, '相': 104, '谁': 105, '没': 106, '跳': 107, '！': 108, '來': 109, '吃': 110, '儿': 111, '醒': 112, '沒': 113, '迷': 114, '心': 115, '找': 116, '把': 117, '就': 118, '当': 119, '信': 120, '忙': 121, '能': 122, '着': 123, '飽': 124, '的': 125, '老': 126, '确': 127, '玩': 128, '加': 129, '知': 130, '友': 131, '下': 132, '拿': 133, '干': 134, '努': 135, '到': 136, '人': 137, '善': 138, '入': 139, '等': 140, '閉': 141, '见': 142, '姆': 143, '趴': 144, '留': 145, '问': 146, '去': 147, '汤': 148}\n"
     ]
    }
   ],
   "source": [
    "# 分割英文数据和中文数据\n",
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "ch_data = ['\\t' + line.split('\\t')[1] + '\\n' for line in data]\n",
    "print('英文数据:', en_data[:10])\n",
    "print('中文数据:', ch_data[:10])\n",
    "for char in ch_data[0]:\n",
    "    print(char)\n",
    "\n",
    "# 分别生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "\n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "\n",
    "print('英文字典:\\n', en2id)\n",
    "print('中文字典共计:\\n', (ch2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 转换输入数据格式\n",
    "建立字典后，将文本数据映射为数字数据形式，并整理为矩阵格式。在生成之前需要考虑训练该模型所需的数据格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38, 8, 20], [38, 8, 20], [23, 35, 1, 20], [18, 34, 8, 30, 41], [38, 25, 44, 44, 39, 41]]\n",
      "[[15, 21, 13, 23], [15, 37, 62, 13, 23], [15, 37, 0, 56, 125, 13, 23], [15, 140, 140, 108, 23], [15, 37, 62, 13, 23]]\n",
      "[[21, 13, 23], [37, 62, 13, 23], [37, 0, 56, 125, 13, 23], [140, 140, 108, 23], [37, 62, 13, 23]]\n"
     ]
    }
   ],
   "source": [
    "# number data\n",
    "\n",
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] for line in ch_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in ch_data]\n",
    "\n",
    "print(en_num_data[:5])\n",
    "print(ch_num_data[:5])\n",
    "print(de_num_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 整理训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "11\n",
      "(100, 11, 149)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# max length\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)\n",
    "\n",
    "\n",
    "encoder_input_data = [line + [0] * (max_encoder_seq_length-len(line)) for line in en_num_data]\n",
    "\n",
    "# no padding, onehot\n",
    "encoder_input_onehot = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
    "decoder_input_onehot = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_onehot[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_onehot[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======预定义模型参数========\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择与建模\n",
    "### 4.1 encoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ======================================keras model==================================\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Embedding, Masking\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, return_state=True)(emb_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 decoder建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============decoder=============\n",
    "decoder_inputs = Input(shape=(None, CH_VOCAB_SIZE))\n",
    "\n",
    "#emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=CH_VOCAB_SIZE, mask_zero=True)(decoder_inputs)\n",
    "lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(CH_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "decoder_h1, _, _ = lstm1(decoder_inputs, initial_state=[encoder_state_h1, encoder_state_c1])\n",
    "decoder_outputs = decoder_dense(decoder_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    11776       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 149)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  415744      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 149)    38293       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 991,125\n",
      "Trainable params: 991,125\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 2.3106 - acc: 0.0000e+00 - val_loss: 2.4389 - val_acc: 0.1182\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 2.2866 - acc: 0.1010 - val_loss: 2.4065 - val_acc: 0.0909\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 2.2465 - acc: 0.0909 - val_loss: 2.3059 - val_acc: 0.0909\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 2.1328 - acc: 0.0909 - val_loss: 1.9673 - val_acc: 0.0909\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.7981 - acc: 0.0909 - val_loss: 2.1700 - val_acc: 0.0909\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.9443 - acc: 0.0909 - val_loss: 1.9938 - val_acc: 0.1636\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.7640 - acc: 0.1384 - val_loss: 1.8792 - val_acc: 0.0818\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6511 - acc: 0.0828 - val_loss: 1.8881 - val_acc: 0.0818\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6579 - acc: 0.0889 - val_loss: 1.9177 - val_acc: 0.1455\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6791 - acc: 0.1626 - val_loss: 1.9206 - val_acc: 0.1182\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6691 - acc: 0.1303 - val_loss: 1.9064 - val_acc: 0.1000\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6376 - acc: 0.1212 - val_loss: 1.8926 - val_acc: 0.1000\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.6023 - acc: 0.1182 - val_loss: 1.8923 - val_acc: 0.1000\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5762 - acc: 0.1182 - val_loss: 1.9088 - val_acc: 0.1000\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5633 - acc: 0.1141 - val_loss: 1.9337 - val_acc: 0.1000\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5563 - acc: 0.1131 - val_loss: 1.9560 - val_acc: 0.1182\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5454 - acc: 0.1242 - val_loss: 1.9704 - val_acc: 0.1364\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5280 - acc: 0.1414 - val_loss: 1.9744 - val_acc: 0.1364\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.5075 - acc: 0.1485 - val_loss: 1.9683 - val_acc: 0.1091\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4939 - acc: 0.1303 - val_loss: 1.9609 - val_acc: 0.1091\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4850 - acc: 0.1273 - val_loss: 1.9601 - val_acc: 0.1364\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4644 - acc: 0.1465 - val_loss: 1.9704 - val_acc: 0.1364\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4451 - acc: 0.1404 - val_loss: 1.9886 - val_acc: 0.1273\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4341 - acc: 0.1354 - val_loss: 2.0081 - val_acc: 0.1273\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4246 - acc: 0.1323 - val_loss: 2.0257 - val_acc: 0.1273\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.4096 - acc: 0.1354 - val_loss: 2.0422 - val_acc: 0.1364\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.3875 - acc: 0.1414 - val_loss: 2.0619 - val_acc: 0.1364\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.3622 - acc: 0.1566 - val_loss: 2.0905 - val_acc: 0.1364\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.3426 - acc: 0.1606 - val_loss: 2.1285 - val_acc: 0.1182\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.3320 - acc: 0.1556 - val_loss: 2.1622 - val_acc: 0.1364\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.3138 - acc: 0.1576 - val_loss: 2.1837 - val_acc: 0.1455\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.2853 - acc: 0.1707 - val_loss: 2.1949 - val_acc: 0.1364\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.2619 - acc: 0.1687 - val_loss: 2.1932 - val_acc: 0.1273\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.2401 - acc: 0.1707 - val_loss: 2.1855 - val_acc: 0.1273\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.2157 - acc: 0.1717 - val_loss: 2.1975 - val_acc: 0.1273\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.1937 - acc: 0.1758 - val_loss: 2.2532 - val_acc: 0.1273\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.1687 - acc: 0.1798 - val_loss: 2.3187 - val_acc: 0.1273\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.1457 - acc: 0.1788 - val_loss: 2.3467 - val_acc: 0.1273\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.1183 - acc: 0.1838 - val_loss: 2.3332 - val_acc: 0.1182\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.0891 - acc: 0.1889 - val_loss: 2.3317 - val_acc: 0.1182\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.0651 - acc: 0.1939 - val_loss: 2.3944 - val_acc: 0.1091\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.0348 - acc: 0.1899 - val_loss: 2.4487 - val_acc: 0.1091\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 1.0081 - acc: 0.1899 - val_loss: 2.4435 - val_acc: 0.1091\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.9791 - acc: 0.2000 - val_loss: 2.4451 - val_acc: 0.1000\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.9514 - acc: 0.2051 - val_loss: 2.4821 - val_acc: 0.1000\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.9239 - acc: 0.2051 - val_loss: 2.4748 - val_acc: 0.1091\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.8913 - acc: 0.2182 - val_loss: 2.4801 - val_acc: 0.1091\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.8641 - acc: 0.2313 - val_loss: 2.5124 - val_acc: 0.1182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.8383 - acc: 0.2253 - val_loss: 2.4542 - val_acc: 0.1091\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.8097 - acc: 0.2444 - val_loss: 2.5152 - val_acc: 0.1273\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.7769 - acc: 0.2394 - val_loss: 2.5127 - val_acc: 0.1273\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.7432 - acc: 0.2586 - val_loss: 2.5161 - val_acc: 0.1273\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.7126 - acc: 0.2747 - val_loss: 2.5584 - val_acc: 0.1273\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.6868 - acc: 0.2697 - val_loss: 2.5343 - val_acc: 0.1273\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.6629 - acc: 0.2990 - val_loss: 2.6209 - val_acc: 0.1182\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.6495 - acc: 0.2697 - val_loss: 2.5754 - val_acc: 0.1182\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.6022 - acc: 0.3212 - val_loss: 2.5814 - val_acc: 0.1364\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5707 - acc: 0.3242 - val_loss: 2.6327 - val_acc: 0.1273\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5518 - acc: 0.3081 - val_loss: 2.5995 - val_acc: 0.1182\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5104 - acc: 0.3414 - val_loss: 2.5797 - val_acc: 0.1364\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4882 - acc: 0.3576 - val_loss: 2.6337 - val_acc: 0.1273\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4627 - acc: 0.3505 - val_loss: 2.6337 - val_acc: 0.1182\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4289 - acc: 0.3697 - val_loss: 2.6037 - val_acc: 0.1182\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4023 - acc: 0.3899 - val_loss: 2.6242 - val_acc: 0.1091\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3850 - acc: 0.3808 - val_loss: 2.6054 - val_acc: 0.1091\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3483 - acc: 0.4010 - val_loss: 2.5996 - val_acc: 0.1182\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3299 - acc: 0.4051 - val_loss: 2.6344 - val_acc: 0.1182\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3115 - acc: 0.4081 - val_loss: 2.6231 - val_acc: 0.1091\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2800 - acc: 0.4182 - val_loss: 2.6335 - val_acc: 0.1000\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2686 - acc: 0.4253 - val_loss: 2.6945 - val_acc: 0.1273\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2485 - acc: 0.4232 - val_loss: 2.6851 - val_acc: 0.1091\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2256 - acc: 0.4273 - val_loss: 2.6858 - val_acc: 0.1000\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2114 - acc: 0.4323 - val_loss: 2.7354 - val_acc: 0.1091\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1954 - acc: 0.4343 - val_loss: 2.7727 - val_acc: 0.1091\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1797 - acc: 0.4414 - val_loss: 2.8167 - val_acc: 0.1091\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1646 - acc: 0.4434 - val_loss: 2.8657 - val_acc: 0.1182\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1544 - acc: 0.4414 - val_loss: 2.8725 - val_acc: 0.1091\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1387 - acc: 0.4455 - val_loss: 2.8976 - val_acc: 0.1091\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1299 - acc: 0.4455 - val_loss: 2.9478 - val_acc: 0.1273\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1189 - acc: 0.4485 - val_loss: 2.9688 - val_acc: 0.1182\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1097 - acc: 0.4505 - val_loss: 2.9774 - val_acc: 0.1182\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.1016 - acc: 0.4505 - val_loss: 3.0061 - val_acc: 0.1182\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0941 - acc: 0.4485 - val_loss: 3.0253 - val_acc: 0.1182\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0868 - acc: 0.4505 - val_loss: 3.0419 - val_acc: 0.1182\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0808 - acc: 0.4525 - val_loss: 3.0674 - val_acc: 0.1273\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0747 - acc: 0.4525 - val_loss: 3.0806 - val_acc: 0.1273\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0697 - acc: 0.4525 - val_loss: 3.0810 - val_acc: 0.1182\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0647 - acc: 0.4525 - val_loss: 3.0997 - val_acc: 0.1182\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0606 - acc: 0.4535 - val_loss: 3.1306 - val_acc: 0.1273\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0565 - acc: 0.4535 - val_loss: 3.1503 - val_acc: 0.1273\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0533 - acc: 0.4535 - val_loss: 3.1614 - val_acc: 0.1182\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0498 - acc: 0.4535 - val_loss: 3.1745 - val_acc: 0.1091\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0474 - acc: 0.4535 - val_loss: 3.1964 - val_acc: 0.1273\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0444 - acc: 0.4535 - val_loss: 3.2156 - val_acc: 0.1273\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0424 - acc: 0.4535 - val_loss: 3.2231 - val_acc: 0.1273\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0399 - acc: 0.4535 - val_loss: 3.2338 - val_acc: 0.1182\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0382 - acc: 0.4535 - val_loss: 3.2528 - val_acc: 0.1364\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0362 - acc: 0.4535 - val_loss: 3.2651 - val_acc: 0.1273\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0348 - acc: 0.4535 - val_loss: 3.2715 - val_acc: 0.1273\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.0331 - acc: 0.4535 - val_loss: 3.2823 - val_acc: 0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 200\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_onehot], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 搭建预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1])\n",
    "\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "decoder_h1, state_h1, state_c1 = lstm1(decoder_inputs, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "decoder_outputs = decoder_dense(decoder_h1)\n",
    "\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1], \n",
    "                      [decoder_outputs, state_h1, state_c1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 利用预测模型进行翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\n",
      "去清洗一下下。\n",
      "\n",
      "Hi.\n",
      "去清洗一下下。\n",
      "\n",
      "Run.\n",
      "不吧。\n",
      "\n",
      "Wait!\n",
      "往后退点。\n",
      "\n",
      "Hello!\n",
      "去清洗一下下。\n",
      "\n",
      "I try.\n",
      "我一下。\n",
      "\n",
      "I won!\n",
      "我一下。\n",
      "\n",
      "Oh no!\n",
      "跳跳跳跳跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Cheers!\n",
      "往前开。\n",
      "\n",
      "He ran.\n",
      "去清洗一下下。\n",
      "\n",
      "Hop in.\n",
      "去清洗一下下。\n",
      "\n",
      "I lost.\n",
      "我一下。\n",
      "\n",
      "I quit.\n",
      "我一下。\n",
      "\n",
      "I'm OK.\n",
      "我一下。\n",
      "\n",
      "Listen.\n",
      "我一下。\n",
      "\n",
      "No way!\n",
      "不吧。\n",
      "\n",
      "No way!\n",
      "不吧。\n",
      "\n",
      "Really?\n",
      "不吧。\n",
      "\n",
      "Try it.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "We try.\n",
      "往后退点。\n",
      "\n",
      "Why me?\n",
      "往后退点。\n",
      "\n",
      "Ask Tom.\n",
      "閉嘴！\n",
      "\n",
      "Be calm.\n",
      "把把他抓跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Be fair.\n",
      "把把他抓跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Be kind.\n",
      "把把他抓跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Be nice.\n",
      "把把他抓跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Call me.\n",
      "往前开。\n",
      "\n",
      "Call us.\n",
      "往前开。\n",
      "\n",
      "Come in.\n",
      "往前开。\n",
      "\n",
      "Get Tom.\n",
      "为什么是我\n",
      "\n",
      "Get out!\n",
      "为什么是我\n",
      "\n",
      "Go away!\n",
      "为什么是我\n",
      "\n",
      "Go away!\n",
      "为什么是我\n",
      "\n",
      "Go away.\n",
      "为什么是我\n",
      "\n",
      "Goodbye!\n",
      "为什么是我\n",
      "\n",
      "Goodbye!\n",
      "为什么是我\n",
      "\n",
      "Hang on!\n",
      "去清洗一下下。\n",
      "\n",
      "He came.\n",
      "去清洗一下下。\n",
      "\n",
      "He runs.\n",
      "去清洗一下下。\n",
      "\n",
      "Help me.\n",
      "去清洗一下下。\n",
      "\n",
      "Hold on.\n",
      "去清洗一下下。\n",
      "\n",
      "Hug Tom.\n",
      "去清洗一下下。\n",
      "\n",
      "I agree.\n",
      "我一下。\n",
      "\n",
      "I'm ill.\n",
      "我一下。\n",
      "\n",
      "I'm old.\n",
      "我一下。\n",
      "\n",
      "It's OK.\n",
      "我一下。\n",
      "\n",
      "It's me.\n",
      "我一下。\n",
      "\n",
      "Join us.\n",
      "加入往往往往往往后抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Keep it.\n",
      "去问汤姆。\n",
      "\n",
      "Kiss me.\n",
      "去问汤姆。\n",
      "\n",
      "Perfect!\n",
      "不来吧。\n",
      "\n",
      "See you.\n",
      "为什么？\n",
      "\n",
      "Shut up!\n",
      "为什么？\n",
      "\n",
      "Skip it.\n",
      "为什么？\n",
      "\n",
      "Take it.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Wake up!\n",
      "往后退点。\n",
      "\n",
      "Wash up.\n",
      "往后退点。\n",
      "\n",
      "We know.\n",
      "往后退点。\n",
      "\n",
      "Welcome.\n",
      "往后退点。\n",
      "\n",
      "Who won?\n",
      "往后退点。\n",
      "\n",
      "Why not?\n",
      "往后退点。\n",
      "\n",
      "You run.\n",
      "去问汤姆。\n",
      "\n",
      "Back off.\n",
      "把把他抓跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Be still.\n",
      "把把他抓跳抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Cuff him.\n",
      "往前开。\n",
      "\n",
      "Drive on.\n",
      "往后退点点。\n",
      "\n",
      "Get away!\n",
      "为什么是我\n",
      "\n",
      "Get away!\n",
      "为什么是我\n",
      "\n",
      "Get down!\n",
      "为什么是我\n",
      "\n",
      "Get lost!\n",
      "为什么是我\n",
      "\n",
      "Get real.\n",
      "为什么是我\n",
      "\n",
      "Grab Tom.\n",
      "为什么是我\n",
      "\n",
      "Grab him.\n",
      "为什么是我\n",
      "\n",
      "Have fun.\n",
      "去清洗一下下。\n",
      "\n",
      "He tries.\n",
      "去清洗一下下。\n",
      "\n",
      "Humor me.\n",
      "去清洗一下下。\n",
      "\n",
      "Hurry up.\n",
      "去清洗一下下。\n",
      "\n",
      "Hurry up.\n",
      "去清洗一下下。\n",
      "\n",
      "I forgot.\n",
      "我一下。\n",
      "\n",
      "I resign.\n",
      "我一下。\n",
      "\n",
      "I'll pay.\n",
      "我一下。\n",
      "\n",
      "I'm busy.\n",
      "我一下。\n",
      "\n",
      "I'm cold.\n",
      "我一下。\n",
      "\n",
      "I'm fine.\n",
      "我一下。\n",
      "\n",
      "I'm full.\n",
      "我一下。\n",
      "\n",
      "I'm sick.\n",
      "我一下。\n",
      "\n",
      "I'm sick.\n",
      "我一下。\n",
      "\n",
      "Leave me.\n",
      "我一下。\n",
      "\n",
      "Let's go!\n",
      "我一下。\n",
      "\n",
      "Let's go!\n",
      "我一下。\n",
      "\n",
      "Let's go!\n",
      "我一下。\n",
      "\n",
      "Look out!\n",
      "我一下。\n",
      "\n",
      "She runs.\n",
      "为什么？\n",
      "\n",
      "Stand up.\n",
      "为什么？\n",
      "\n",
      "They won.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Tom died.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Tom quit.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Tom swam.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Trust me.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n",
      "Try hard.\n",
      "为什么抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓抓\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(100):\n",
    "    test_data = encoder_input_data[k:k+1]\n",
    "    h1, c1 = encoder_model.predict(test_data)\n",
    "    target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "    target_seq[0, 0, ch2id['\\t']] = 1\n",
    "    outputs = []\n",
    "    while True:\n",
    "        output_tokens, h1, c1 = decoder_model.predict([target_seq, h1, c1])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        outputs.append(sampled_token_index)\n",
    "        target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        if sampled_token_index == ch2id['\\n'] or len(outputs) > 20: break\n",
    "    \n",
    "    print(en_data[k])\n",
    "    print(''.join([id2ch[i] for i in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
