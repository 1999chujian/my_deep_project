{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于seq2seq的中英文翻译系统\n",
    "## 1. 项目背景\n",
    "之前我们利用lstm进行建模，设计了一个自动生成莫言小说的模型，这次想要利用rnn的特点搭建一个中英文的翻译系统。传统的RNN输入和输出长度要一致，而seq2seq在RNN的基础上进行改进，实现了变长序列的输入和输出，广泛的应用在了机器翻译、对话系统、文本摘要等领域。 \n",
    "- 代码参考：https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "## 2. 项目数据\n",
    "项目数据使用中英文翻译数据集，来实现字符级的seq2seq模型的训练。 \n",
    "该文件来自于:http://www.manythings.org/anki/\n",
    "\n",
    "内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\\t如果一個人在成人前沒有機會習得目標語言，他對該語言的認識達到母語者程度的機會是相當小的。\"]\n"
     ]
    }
   ],
   "source": [
    "# ========读取原始数据========\n",
    "with open('cmn.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "print(data[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据处理\n",
    "### 3.1 生成字典\n",
    "我们需要将汉字和英文映射为能够输入到模型中的数字信息，就需要建立一个映射关系，需要生成汉字和数字互相映射的字典。\n",
    "- 我们将英文按照每个字母对应一个id\n",
    "- 我们将中文按照每一个汉字对应一个id\n",
    "- **注意增加：**\n",
    "    1. 未知符号：UNK\n",
    "    2. 补齐符号：PAD\n",
    "    3. 开始符号：GO\n",
    "    4. 结束符号：EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文数据: ['Hi.', 'Hi.', 'Run.', 'Wait!', 'Hello!', 'I try.', 'I won!', 'Oh no!', 'Cheers!', 'He ran.']\n",
      "中文数据: ['嗨。', '你好。', '你用跑的。', '等等！', '你好。', '让我来。', '我赢了。', '不会吧。', '乾杯!', '他跑了。']\n",
      "英文字典:\n",
      " {'__UNK__': 0, '__PAD__': 1, 'x': 2, 'A': 3, 'c': 4, 'E': 5, '0': 6, 'Q': 7, 'é': 8, 'T': 9, ' ': 10, 'm': 11, 'F': 12, '$': 13, 'P': 14, 'q': 15, 'V': 16, 'k': 17, 'N': 18, '.': 19, 'U': 20, 'L': 21, 'i': 22, 'Z': 23, 'r': 24, 'M': 25, 'u': 26, '6': 27, 'y': 28, '2': 29, '?': 30, '!': 31, 'D': 32, '5': 33, '4': 34, 'O': 35, 'h': 36, 'p': 37, 'Y': 38, 'J': 39, 'S': 40, '3': 41, 'I': 42, 'o': 43, 'K': 44, 'b': 45, 'l': 46, '1': 47, 'g': 48, 'j': 49, '+': 50, 'f': 51, ',': 52, 'G': 53, ':': 54, '7': 55, '\"': 56, '9': 57, '’': 58, 'C': 59, '%': 60, 'e': 61, 'R': 62, 'n': 63, 'w': 64, 'W': 65, 'a': 66, 's': 67, \"'\": 68, 'd': 69, 'B': 70, '8': 71, '-': 72, 'z': 73, 't': 74, 'H': 75, 'v': 76}\n",
      "中文字典共计: 3421\n"
     ]
    }
   ],
   "source": [
    "# 分割英文数据和中文数据\n",
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "ch_data = [line.split('\\t')[1] for line in data]\n",
    "print('英文数据:', en_data[:10])\n",
    "print('中文数据:', ch_data[:10])\n",
    "\n",
    "# 分别生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = ['__UNK__', '__PAD__'] + list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = ['__UNK__', '__PAD__', '__GO__', '__EOS__'] + list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "print('英文字典:\\n', en2id)\n",
    "print('中文字典共计:', len(ch2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 转换输入数据格式\n",
    "建立字典后，将文本数据映射为数字数据形式，并整理为矩阵格式。在生成之前需要考虑训练该模型所需的数据格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75, 22, 19], [75, 22, 19], [62, 26, 63, 19], [65, 66, 22, 74, 31], [75, 61, 46, 46, 43, 31]]\n",
      "[[1379, 546, 3], [2250, 295, 546, 3], [2250, 563, 2217, 531, 546, 3], [2197, 2197, 1914, 3], [2250, 295, 546, 3]]\n"
     ]
    }
   ],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] + [ch2id['__EOS__']] for line in ch_data]\n",
    "print(en_num_data[:5])\n",
    "print(ch_num_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======预定义模型参数========\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "BATCH_NUMS = len(en_num_data) // (BATCH_SIZE)\n",
    "HIDDEN_SIZE = 512\n",
    "HIDDEN_LAYERS = 3\n",
    "MAX_GRAD_NORM = 1\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择与建模\n",
    "### 4.1 encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================keras model==================================\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Embedding\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE, input_length=None)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)(emb_inp)\n",
    "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, activation='relu', return_state=True, dropout=0.2)(encoder_h1)\n",
    "encoder_state = [[encoder_state_h1, encoder_state_c1], [encoder_state_h2, encoder_state_c2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============decoder=============\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=CH_VOCAB_SIZE, input_length=None)(decoder_inputs)\n",
    "decoder_h1 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)(emb_target, initial_state=encoder_states[0])\n",
    "decoder_h2 = LSTM(HIDDEN_SIZE, activation='relu', return_sequences=True, return_state=True, dropout=0.2)(decoder_h1, initial_state=encoder_states[1])\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
