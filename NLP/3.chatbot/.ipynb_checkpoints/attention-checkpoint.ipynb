{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0,1,2,3,4,5,6]\n",
    "data_x = np.array([[3],[4],[5]])\n",
    "data_y = np.array([[0,2,3,6,1],[0,2,4,6,1],[0,2,5,6,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tf.placeholder(dtype=tf.int32,shape=[None,None])\n",
    "len_input = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "target = tf.placeholder(dtype=tf.int32,shape=[None,None])\n",
    "len_target = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "batch_size = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "max_target_sequence_len = tf.reduce_max(len_target,name=\"max_target_len\")\n",
    "mask = tf.sequence_mask(lengths=len_target,\n",
    "                        maxlen=max_target_sequence_len,\n",
    "                        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_cell(layer_num=5):\n",
    "    def single_rnn_cell():\n",
    "        single_cell = tf.contrib.rnn.LSTMCell(10)\n",
    "        #添加dropout\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(single_cell, output_keep_prob=0.8)\n",
    "        return cell\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([single_rnn_cell() for _ in range(layer_num)])\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(name=\"embedding\",shape=[7,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_input = tf.nn.embedding_lookup(embedding,input_)\n",
    "embedded_target = tf.nn.embedding_lookup(embedding,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = create_rnn_cell()\n",
    "encoder_output, encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell,\n",
    "                                                  dtype=tf.float32,\n",
    "                                                  inputs=embedded_input,\n",
    "                                                  sequence_length=len_input,\n",
    "                                                  time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper at 0x23e29d17908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mechianism = tf.contrib.seq2seq.BahdanauAttention(num_units=2,\n",
    "                                                            memory=encoder_output,\n",
    "                                                            memory_sequence_length=len_input)\n",
    "decoder_cell = create_rnn_cell()\n",
    "decoder_cell = tf.contrib.seq2seq.AttentionWrapper(cell=decoder_cell,attention_layer_size=2,\n",
    "                                                   attention_mechanism=attention_mechianism,)\n",
    "decoder_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = decoder_cell.zero_state(batch_size[0],dtype=tf.float32).clone(cell_state=encoder_state)\n",
    "output_layer = Dense(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_cell = tf.contrib.rnn.LSTMCell(2)\n",
    "embedded_target_transpose = tf.transpose(embedded_target, perm=[1, 0, 2])\n",
    "\n",
    "training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=embedded_target_transpose,\n",
    "                                                    time_major=True,\n",
    "                                                    sequence_length=len_target,\n",
    "                                                    name=\"Train_helper\")\n",
    "training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,output_layer=output_layer,\n",
    "                                                   initial_state=decoder_initial_state,\n",
    "                                                   helper=training_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                         maximum_iterations=max_target_sequence_len,\n",
    "                                                         impute_finished=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_logits_train = tf.identity(decoder_output.rnn_output)\n",
    "decoder_predict_train = tf.argmax(decoder_logits_train,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.contrib.seq2seq.sequence_loss(logits=decoder_logits_train,targets=target,weights=mask)\n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(0.1)\n",
    "# trainable_params = tf.trainable_variables()\n",
    "# gradients = tf.gradients(loss, trainable_params)\n",
    "# clip_gradients, _ = tf.clip_by_global_norm(gradients, 0.5)\n",
    "# train_op = optimizer.apply_gradients(zip(clip_gradients, trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.071101565\r"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    loss_,_ = sess.run([loss,optimizer],{input_:data_x,target:data_y,len_input:[1,1,1],len_target:np.array([5,5,5]),batch_size:[3]})\n",
    "    print(loss_,end=\"\\r\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tf.ones([batch_size[0]], tf.int32) * 0\n",
    "decoder_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=embedding,\n",
    "                                                           start_tokens=start,\n",
    "                                                           end_token=1)\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,output_layer=output_layer,\n",
    "                                                    helper=decoder_helper,\n",
    "                                                    initial_state=decoder_initial_state)\n",
    "# decoder_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder,\n",
    "#                                                         maximum_iterations=10)\n",
    "# inference_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs,_,_= tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder,\n",
    "                                                        maximum_iterations=10)\n",
    "decoder_predict_decode = tf.expand_dims(decoder_outputs.sample_id, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.array([[3],[4],[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0],\n",
       "         [2],\n",
       "         [3],\n",
       "         [6],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [2],\n",
       "         [4],\n",
       "         [6],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [2],\n",
       "         [5],\n",
       "         [6],\n",
       "         [1]]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = sess.run([decoder_predict_decode],{input_:data_test,len_input:[len(data_test[0]),len(data_test[0]),len(data_test[0])],batch_size:[3]})\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TextLineDataset shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "\n",
    "src_vocab_table = lookup_ops.index_table_from_file('answer', default_value=0)\n",
    "src_dataset = tf.data.TextLineDataset(tf.gfile.Glob('answer'))\n",
    "\n",
    "print(src_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shandong\n"
     ]
    }
   ],
   "source": [
    "def create_hparams():\n",
    "    return tf.contrib.training.HParams(\n",
    "        add='shandong',\n",
    "        name='sunhongwen')\n",
    "\n",
    "params = create_hparams()\n",
    "print(params.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cee72ba08d46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluation_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'inference'"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"TensorFlow NMT model implementation.\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "def add_arguments(parser):\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    parser.add_argument(\"--num_units\", type=int, default=32, help=\"Network size.\")\n",
    "\n",
    "    \n",
    "nmt_parser = argparse.ArgumentParser()\n",
    "add_arguments(nmt_parser)\n",
    "print(nmt_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
